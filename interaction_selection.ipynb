{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# essential imports\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from lda import evaluate_acc\n",
    "from lda import lda\n",
    "from lda import kfold_index\n",
    "from lda import standardize\n",
    "from lda import normalize\n",
    "# chi2 user\n",
    "from lda import chi2\n",
    "from logreg import LogReg\n",
    "# chi2 scikit\n",
    "# from sklearn.feature_selection import chi2\n",
    "\n",
    "# LDA test\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated test (normalized, first 11 features are kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction is 1-way:  1\n",
      "number of features:  3\n",
      "p value:  0.01\n",
      "1-way interaction\n",
      "mean acc:  0.7378965517241381\n",
      "std acc:  0.019820140659114742\n",
      "\n",
      "interaction is 1-way:  1\n",
      "number of features:  5\n",
      "p value:  0.05\n",
      "1-way interaction\n",
      "mean acc:  0.7415848354231974\n",
      "std acc:  0.01899379751806374\n",
      "\n",
      "interaction is 1-way:  1\n",
      "number of features:  5\n",
      "p value:  0.1\n",
      "1-way interaction\n",
      "mean acc:  0.7422815438871473\n",
      "std acc:  0.02568205174570555\n",
      "\n",
      "interaction is 1-way:  1\n",
      "number of features:  6\n",
      "p value:  0.25\n",
      "1-way interaction\n",
      "mean acc:  0.7419110501567399\n",
      "std acc:  0.02103671362408487\n",
      "\n",
      "interaction is 1-way:  1\n",
      "number of features:  9\n",
      "p value:  0.5\n",
      "1-way interaction\n",
      "mean acc:  0.7427752742946709\n",
      "std acc:  0.020312883719775764\n",
      "\n",
      "interaction is 1-way:  1\n",
      "number of features:  11\n",
      "p value:  1\n",
      "1-way interaction\n",
      "mean acc:  0.7420916927899688\n",
      "std acc:  0.017492340438041834\n",
      "\n",
      "number of features:  24\n",
      "p value:  0.01\n",
      "2-way interaction\n",
      "mean acc:  0.7427147335423198\n",
      "std acc:  0.022670406240661117\n",
      "\n",
      "number of features:  33\n",
      "p value:  0.05\n",
      "2-way interaction\n",
      "mean acc:  0.7439034090909091\n",
      "std acc:  0.020620547485062164\n",
      "\n",
      "number of features:  38\n",
      "p value:  0.1\n",
      "2-way interaction\n",
      "mean acc:  0.7449094827586206\n",
      "std acc:  0.01978274391879806\n",
      "\n",
      "number of features:  43\n",
      "p value:  0.25\n",
      "2-way interaction\n",
      "mean acc:  0.7440905172413794\n",
      "std acc:  0.019447719999713897\n",
      "\n",
      "number of features:  51\n",
      "p value:  0.5\n",
      "2-way interaction\n",
      "mean acc:  0.744216105015674\n",
      "std acc:  0.021896236963376364\n",
      "\n",
      "number of features:  66\n",
      "p value:  1\n",
      "2-way interaction\n",
      "mean acc:  0.7466557601880878\n",
      "std acc:  0.023037190850849987\n",
      "\n",
      "number of features:  56\n",
      "p value:  0.01\n",
      "3-way interaction\n",
      "mean acc:  0.7431479231974921\n",
      "std acc:  0.020034126744164733\n",
      "\n",
      "number of features:  83\n",
      "p value:  0.05\n",
      "3-way interaction\n",
      "mean acc:  0.7460164576802508\n",
      "std acc:  0.024206460043041632\n",
      "\n",
      "number of features:  103\n",
      "p value:  0.1\n",
      "3-way interaction\n",
      "mean acc:  0.7457172805642632\n",
      "std acc:  0.020514134139651717\n",
      "\n",
      "number of features:  135\n",
      "p value:  0.25\n",
      "3-way interaction\n",
      "mean acc:  0.744530172413793\n",
      "std acc:  0.022331234462586447\n",
      "\n",
      "number of features:  166\n",
      "p value:  0.5\n",
      "3-way interaction\n",
      "mean acc:  0.7455268416927902\n",
      "std acc:  0.024849891456018337\n",
      "\n",
      "number of features:  231\n",
      "p value:  1\n",
      "3-way interaction\n",
      "mean acc:  0.7467809561128527\n",
      "std acc:  0.01987763418900962\n",
      "\n",
      "number of features:  105\n",
      "p value:  0.01\n",
      "4-way interaction\n",
      "mean acc:  0.7434012539184954\n",
      "std acc:  0.020328873292449447\n",
      "\n",
      "number of features:  161\n",
      "p value:  0.05\n",
      "4-way interaction\n",
      "mean acc:  0.747587578369906\n",
      "std acc:  0.02513163356434918\n",
      "\n",
      "number of features:  209\n",
      "p value:  0.1\n",
      "4-way interaction\n",
      "mean acc:  0.7480344827586207\n",
      "std acc:  0.02130854619159352\n",
      "\n",
      "number of features:  303\n",
      "p value:  0.25\n",
      "4-way interaction\n",
      "mean acc:  0.7509096786833857\n",
      "std acc:  0.020622046866248682\n",
      "\n",
      "number of features:  391\n",
      "p value:  0.5\n",
      "4-way interaction\n",
      "mean acc:  0.7517817398119122\n",
      "std acc:  0.019134782865507984\n",
      "\n",
      "number of features:  561\n",
      "p value:  1\n",
      "4-way interaction\n",
      "mean acc:  0.7153183777429466\n",
      "std acc:  0.024119511233106683\n",
      "\n",
      "number of features:  152\n",
      "p value:  0.01\n",
      "5-way interaction\n",
      "mean acc:  0.7450250783699058\n",
      "std acc:  0.021536771704618478\n",
      "\n",
      "number of features:  254\n",
      "p value:  0.05\n",
      "5-way interaction\n",
      "mean acc:  0.7504086990595613\n",
      "std acc:  0.02129980301406349\n",
      "\n",
      "number of features:  338\n",
      "p value:  0.1\n",
      "5-way interaction\n",
      "mean acc:  0.7514012539184952\n",
      "std acc:  0.02407554936856617\n",
      "\n",
      "number of features:  518\n",
      "p value:  0.25\n",
      "5-way interaction\n",
      "mean acc:  0.7419038009404388\n",
      "std acc:  0.018533766895559675\n",
      "\n",
      "number of features:  704\n",
      "p value:  0.5\n",
      "5-way interaction\n",
      "mean acc:  0.7183857758620691\n",
      "std acc:  0.021979388324535484\n",
      "\n",
      "number of features:  1023\n",
      "p value:  1\n",
      "5-way interaction\n",
      "mean acc:  0.6874259404388715\n",
      "std acc:  0.02382041069654047\n",
      "\n",
      "number of features:  180\n",
      "p value:  0.01\n",
      "6-way interaction\n",
      "mean acc:  0.7443371865203762\n",
      "std acc:  0.01950430369361858\n",
      "\n",
      "number of features:  337\n",
      "p value:  0.05\n",
      "6-way interaction\n",
      "mean acc:  0.7502772335423197\n",
      "std acc:  0.021359535300074808\n",
      "\n",
      "number of features:  457\n",
      "p value:  0.1\n",
      "6-way interaction\n",
      "mean acc:  0.7535354623824451\n",
      "std acc:  0.024081050624832092\n",
      "\n",
      "number of features:  719\n",
      "p value:  0.25\n",
      "6-way interaction\n",
      "mean acc:  0.7337690047021945\n",
      "std acc:  0.01999577502292222\n",
      "\n",
      "number of features:  1024\n",
      "p value:  0.5\n",
      "6-way interaction\n",
      "mean acc:  0.7056892633228842\n",
      "std acc:  0.0261433525866892\n",
      "\n",
      "number of features:  1485\n",
      "p value:  1\n",
      "6-way interaction\n",
      "mean acc:  0.6834288793103448\n",
      "std acc:  0.020635191273444285\n",
      "\n",
      "((0.1, 6, array([[0, 1.042185209008817, 0.3073137941896489],\n",
      "       [1, 8.891043306316842, 0.0028657293536039487],\n",
      "       [2, 5.666702872536845, 0.01728992371507948],\n",
      "       ...,\n",
      "       [(2, 4, 5, 6, 7, 8), 2.870358635193864, 0.09022476239241436],\n",
      "       [(4, 5, 6, 7, 8, 9), 3.25041673951357, 0.07140530067667131],\n",
      "       [(4, 5, 6, 7, 8, 10), 5.130657361019558, 0.023506823286095192]],\n",
      "      dtype=object)), 0.7535354623824451)\n",
      "best acc for below:  0.7535354623824451\n",
      "number of features:  457\n",
      "p value:  0.1\n",
      "6-way interaction\n",
      "features chi/pval:  [[0 1.042185209008817 0.3073137941896489]\n",
      " [1 8.891043306316842 0.0028657293536039487]\n",
      " [2 5.666702872536845 0.01728992371507948]\n",
      " ...\n",
      " [(2, 4, 5, 6, 7, 8) 2.870358635193864 0.09022476239241436]\n",
      " [(4, 5, 6, 7, 8, 9) 3.25041673951357 0.07140530067667131]\n",
      " [(4, 5, 6, 7, 8, 10) 5.130657361019558 0.023506823286095192]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# essential imports\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from lda import evaluate_acc\n",
    "from lda import lda\n",
    "from lda import kfold_index\n",
    "from lda import standardize\n",
    "from lda import normalize\n",
    "\n",
    "# chi2 user\n",
    "from lda import chi2\n",
    "from logreg import LogReg\n",
    "\n",
    "# chi2 scikit\n",
    "# from sklearn.feature_selection import chi2\n",
    "\n",
    "# LDA test\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "dlist = []\n",
    "pvallist = [0.01, 0.05, 0.1, 0.25, 0.5, 1]\n",
    "interactionT = [1, 2, 3, 4,5,6]\n",
    "\n",
    "# Load datasets\n",
    "# 1599 red win\n",
    "wine_path = \"./Data/winequality-red.csv\"\n",
    "data = np.genfromtxt(wine_path, delimiter=\";\", skip_header=1)\n",
    "\n",
    "for interaction in interactionT:\n",
    "    for pvali in pvallist:\n",
    "\n",
    "        X_wine = data[:, 0:11]  # select columns 1 through 11\n",
    "        y_wine = data[:, 11]\n",
    "\n",
    "        # convert y values to 0, 1\n",
    "        y_wine = np.where(y_wine < 5.5, 0, 1)\n",
    "\n",
    "        # standardize\n",
    "        # X_wine = standardize(X_wine)\n",
    "\n",
    "        # adding interaction terms\n",
    "        from itertools import combinations\n",
    "\n",
    "        list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "        comblist = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "        comb = combinations(comblist, 2)\n",
    "\n",
    "        i = 11\n",
    "        if interaction in [2, 3, 4, 5,6]:\n",
    "            for (a, b) in comb:\n",
    "                # print((a, b), i)\n",
    "                list.append((a, b))\n",
    "                i += 1\n",
    "                inter = X_wine[:, a] * X_wine[:, b]\n",
    "                X_wine = np.column_stack((X_wine, inter))\n",
    "            if interaction in [3, 4, 5,6]:\n",
    "                comb3 = combinations(comblist, 3)\n",
    "                for (a, b, c) in comb3:\n",
    "                    # print((a, b, c), i)\n",
    "                    list.append((a, b, c))\n",
    "                    i += 1\n",
    "                    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c]\n",
    "                    X_wine = np.column_stack((X_wine, inter))\n",
    "                if interaction in [4, 5,6]:\n",
    "                    comb4 = combinations(comblist, 4)\n",
    "                    for (a, b, c, d) in comb4:\n",
    "                        # print((a, b, c, d), i)\n",
    "                        list.append((a, b, c, d))\n",
    "                        i += 1\n",
    "                        inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]\n",
    "                        X_wine = np.column_stack((X_wine, inter))\n",
    "                    if interaction in [5,6]:\n",
    "                        comb5 = combinations(comblist, 5)\n",
    "                        for (a, b, c, d, e) in comb5:\n",
    "                            # print((a, b, c, d, e), i)\n",
    "                            list.append((a, b, c, d, e))\n",
    "                            i += 1\n",
    "                            inter = (\n",
    "                                X_wine[:, a]\n",
    "                                * X_wine[:, b]\n",
    "                                * X_wine[:, c]\n",
    "                                * X_wine[:, d]\n",
    "                                * X_wine[:, e]\n",
    "                            )\n",
    "                            X_wine = np.column_stack((X_wine, inter))\n",
    "                        if interaction == 6:\n",
    "                            comb6 = combinations(comblist, 6)\n",
    "                            for (a, b, c, d, e,f) in comb6:\n",
    "                                # print((a, b, c, d, e,f), i)\n",
    "                                list.append((a, b, c, d, e,f))\n",
    "                                i += 1\n",
    "                                inter = (\n",
    "                                    X_wine[:, a]\n",
    "                                    * X_wine[:, b]\n",
    "                                    * X_wine[:, c]\n",
    "                                    * X_wine[:, d]\n",
    "                                    * X_wine[:, e]\n",
    "                                    * X_wine[:, f]\n",
    "                                )\n",
    "                                X_wine = np.column_stack((X_wine, inter))\n",
    "                            \n",
    "        else:\n",
    "            print(\"interaction is 1-way: \", interaction)\n",
    "\n",
    "        # normalize\n",
    "        X_wine = normalize(X_wine)\n",
    "\n",
    "        # chi squared table todo\n",
    "        chi, pval = chi2(X_wine, y_wine)\n",
    "        list = np.asarray(list)\n",
    "        chilist = np.column_stack((list, chi, pval))\n",
    "\n",
    "        # threshold of chisquare interaction\n",
    "        index_of_chi = np.where(pval > pvali)\n",
    "        # keep 1-way interaction terms when testing 2 way or larger\n",
    "        if interaction in [2, 3, 4, 5,6]:\n",
    "            index_of_chi = np.delete(index_of_chi, np.where(index_of_chi[0] < 11), axis=1)\n",
    "        # remove comment to delete insignificant interaction rows\n",
    "        X_wine = np.delete(X_wine, index_of_chi, axis=1)\n",
    "        chilist = np.delete(chilist, index_of_chi, axis=0)\n",
    "        #print(chilist)\n",
    "        print(\"number of features: \", len(chilist))\n",
    "        print(\"p value: \", pvali)\n",
    "        print(\"%d-way interaction\" % interaction)\n",
    "        y_wine = y_wine.reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "        # kfold test 10 times\n",
    "        score = []\n",
    "        for i in range(10):\n",
    "            for train_index, test_index in kfold_index(5, X_wine):\n",
    "                # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "                y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "                # project lda\n",
    "                ld = LogReg(learning_rate=0.001)\n",
    "                ld.fit(X_train, y_train)\n",
    "                y_pred = ld.predict(X_test)\n",
    "                score.append(evaluate_acc(y_test, y_pred))\n",
    "        print(\"mean acc: \", np.mean(score))\n",
    "        print(\"std acc: \", np.std(score))\n",
    "        print()\n",
    "        acc = np.mean(score)\n",
    "\n",
    "\n",
    "        dlist.append(((pvali,interaction,chilist),acc))\n",
    "\n",
    "\n",
    "\n",
    "#print(dlist)\n",
    "from operator import itemgetter\n",
    "print(max(dlist,key=itemgetter(1)))\n",
    "((pvali,interaction,chilist),acc)=max(dlist,key=itemgetter(1))\n",
    "print(\"best acc for below: \", acc)\n",
    "print(\"number of features: \", len(chilist))\n",
    "print(\"p value: \", pvali)\n",
    "print(\"%d-way interaction\" % interaction)\n",
    "print(\"features chi/pval: \", chilist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated test (not normalized, first 11 features are kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction is 1-way:  1\n",
      "number of features:  4\n",
      "p value:  0.01\n",
      "1-way interaction\n",
      "mean acc:  0.5553546238244514\n",
      "std acc:  0.02012246974148718\n",
      "\n",
      "interaction is 1-way:  1\n",
      "number of features:  6\n",
      "p value:  0.05\n",
      "1-way interaction\n",
      "mean acc:  0.5709737460815048\n",
      "std acc:  0.02130120408507932\n",
      "\n",
      "interaction is 1-way:  1\n",
      "number of features:  7\n",
      "p value:  0.1\n",
      "1-way interaction\n",
      "mean acc:  0.5684952978056426\n",
      "std acc:  0.02071064057708918\n",
      "\n",
      "interaction is 1-way:  1\n",
      "number of features:  7\n",
      "p value:  0.25\n",
      "1-way interaction\n",
      "mean acc:  0.570382053291536\n",
      "std acc:  0.024957124345139563\n",
      "\n",
      "interaction is 1-way:  1\n",
      "number of features:  8\n",
      "p value:  0.5\n",
      "1-way interaction\n",
      "mean acc:  0.5640987460815048\n",
      "std acc:  0.024951934876776397\n",
      "\n",
      "interaction is 1-way:  1\n",
      "number of features:  11\n",
      "p value:  1\n",
      "1-way interaction\n",
      "mean acc:  0.5728702978056426\n",
      "std acc:  0.025767649821316664\n",
      "\n",
      "number of features:  48\n",
      "p value:  0.01\n",
      "2-way interaction\n",
      "mean acc:  0.563489420062696\n",
      "std acc:  0.029772431630639605\n",
      "\n",
      "number of features:  51\n",
      "p value:  0.05\n",
      "2-way interaction\n",
      "mean acc:  0.585382053291536\n",
      "std acc:  0.017378764537005396\n",
      "\n",
      "number of features:  52\n",
      "p value:  0.1\n",
      "2-way interaction\n",
      "mean acc:  0.585380094043887\n",
      "std acc:  0.02434159265113927\n",
      "\n",
      "number of features:  56\n",
      "p value:  0.25\n",
      "2-way interaction\n",
      "mean acc:  0.5822746865203762\n",
      "std acc:  0.035791641798751425\n",
      "\n",
      "number of features:  59\n",
      "p value:  0.5\n",
      "2-way interaction\n",
      "mean acc:  0.5904153605015674\n",
      "std acc:  0.041221336952024276\n",
      "\n",
      "number of features:  66\n",
      "p value:  1\n",
      "2-way interaction\n",
      "mean acc:  0.5972688087774295\n",
      "std acc:  0.02283258942831634\n",
      "\n",
      "number of features:  172\n",
      "p value:  0.01\n",
      "3-way interaction\n",
      "mean acc:  0.6353957680250784\n",
      "std acc:  0.044581408895756215\n",
      "\n",
      "number of features:  184\n",
      "p value:  0.05\n",
      "3-way interaction\n",
      "mean acc:  0.602259012539185\n",
      "std acc:  0.024434225565355766\n",
      "\n",
      "number of features:  186\n",
      "p value:  0.1\n",
      "3-way interaction\n",
      "mean acc:  0.6291516457680251\n",
      "std acc:  0.03938031696523639\n",
      "\n",
      "number of features:  196\n",
      "p value:  0.25\n",
      "3-way interaction\n",
      "mean acc:  0.6253644200626959\n",
      "std acc:  0.041644560898663426\n",
      "\n",
      "number of features:  206\n",
      "p value:  0.5\n",
      "3-way interaction\n",
      "mean acc:  0.6378879310344827\n",
      "std acc:  0.03662681119285375\n",
      "\n",
      "number of features:  231\n",
      "p value:  1\n",
      "3-way interaction\n",
      "mean acc:  0.6504271159874608\n",
      "std acc:  0.02781219537331205\n",
      "\n",
      "number of features:  439\n",
      "p value:  0.01\n",
      "4-way interaction\n",
      "mean acc:  0.6447884012539185\n",
      "std acc:  0.0244270217178542\n",
      "\n",
      "number of features:  465\n",
      "p value:  0.05\n",
      "4-way interaction\n",
      "mean acc:  0.6397727272727273\n",
      "std acc:  0.028486825467338613\n",
      "\n",
      "number of features:  472\n",
      "p value:  0.1\n",
      "4-way interaction\n",
      "mean acc:  0.6448021159874608\n",
      "std acc:  0.054437850034164785\n",
      "\n",
      "number of features:  487\n",
      "p value:  0.25\n",
      "4-way interaction\n",
      "mean acc:  0.6378546238244513\n",
      "std acc:  0.04481204757052042\n",
      "\n",
      "number of features:  513\n",
      "p value:  0.5\n",
      "4-way interaction\n",
      "mean acc:  0.605342868338558\n",
      "std acc:  0.053015203389527574\n",
      "\n",
      "number of features:  561\n",
      "p value:  1\n",
      "4-way interaction\n",
      "mean acc:  0.6122844827586207\n",
      "std acc:  0.030759905430684242\n",
      "\n",
      "number of features:  845\n",
      "p value:  0.01\n",
      "5-way interaction\n",
      "mean acc:  0.6022492163009405\n",
      "std acc:  0.050342010173910975\n",
      "\n",
      "number of features:  880\n",
      "p value:  0.05\n",
      "5-way interaction\n",
      "mean acc:  0.5903565830721003\n",
      "std acc:  0.05144970918659278\n",
      "\n",
      "number of features:  893\n",
      "p value:  0.1\n",
      "5-way interaction\n",
      "mean acc:  0.6453761755485894\n",
      "std acc:  0.041177863990260424\n",
      "\n",
      "number of features:  914\n",
      "p value:  0.25\n",
      "5-way interaction\n",
      "mean acc:  0.6035011755485893\n",
      "std acc:  0.019115502478217587\n",
      "\n",
      "number of features:  960\n",
      "p value:  0.5\n",
      "5-way interaction\n",
      "mean acc:  0.6317025862068966\n",
      "std acc:  0.048501359126303795\n",
      "\n",
      "number of features:  1023\n",
      "p value:  1\n",
      "5-way interaction\n",
      "mean acc:  0.5910246865203762\n",
      "std acc:  0.041868285832400015\n",
      "\n",
      "number of features:  1272\n",
      "p value:  0.01\n",
      "6-way interaction\n",
      "mean acc:  0.5747511755485892\n",
      "std acc:  0.03292275093364209\n",
      "\n",
      "number of features:  1311\n",
      "p value:  0.05\n",
      "6-way interaction\n",
      "mean acc:  0.5928565830721003\n",
      "std acc:  0.03344737897092191\n",
      "\n",
      "number of features:  1327\n",
      "p value:  0.1\n",
      "6-way interaction\n",
      "mean acc:  0.5765967868338558\n",
      "std acc:  0.025740379821725893\n",
      "\n",
      "number of features:  1354\n",
      "p value:  0.25\n",
      "6-way interaction\n",
      "mean acc:  0.5928702978056426\n",
      "std acc:  0.0030522311656057685\n",
      "\n",
      "number of features:  1415\n",
      "p value:  0.5\n",
      "6-way interaction\n",
      "mean acc:  0.5597061128526646\n",
      "std acc:  0.032040895580386995\n",
      "\n",
      "number of features:  1485\n",
      "p value:  1\n",
      "6-way interaction\n",
      "mean acc:  0.5728683385579937\n",
      "std acc:  0.030895980319663445\n",
      "\n",
      "((1, 3, array([[0, 5.265256679580366, 0.02175521435486419],\n",
      "       [1, 10.029710173863258, 0.0015403527221121511],\n",
      "       [2, 5.666702872536871, 0.017289923715079227],\n",
      "       [3, 0.0058402262756335616, 0.9390838660041609],\n",
      "       [4, 0.4851959137615238, 0.48607815988089265],\n",
      "       [5, 42.00590680180157, 9.099812365887488e-11],\n",
      "       [6, 2002.3058996858863, 0.0],\n",
      "       [7, 0.00014457202542018628, 0.9904066181813496],\n",
      "       [8, 0.00012255107490063595, 0.9911673757910429],\n",
      "       [9, 3.3176173522259527, 0.0685410969134076],\n",
      "       [10, 32.90863266810389, 9.659338720612162e-09],\n",
      "       [(0, 1), 61.087791239690276, 5.458559233735258e-15],\n",
      "       [(0, 2), 75.00723709398163, 4.6899177573750446e-18],\n",
      "       [(0, 3), 13.043360434026635, 0.0003043614910347032],\n",
      "       [(0, 4), 1.7102788702259173, 0.19094919251302145],\n",
      "       [(0, 5), 220.4664333194569, 7.155550807351051e-50],\n",
      "       [(0, 6), 14938.361810841634, 0.0],\n",
      "       [(0, 7), 5.119573286175557, 0.02365743331034771],\n",
      "       [(0, 8), 15.554168634264991, 8.017467061618895e-05],\n",
      "       [(0, 9), 53.702478857220626, 2.332687545670866e-13],\n",
      "       [(0, 10), 544.9296772710032, 1.59588116590238e-120],\n",
      "       [(1, 2), 0.0011169711576811876, 0.9733387689898608],\n",
      "       [(1, 3), 25.565192194059833, 4.2768532524227246e-07],\n",
      "       [(1, 4), 1.951076807143802, 0.16247086174795994],\n",
      "       [(1, 5), 262.17641079016204, 5.755963324417165e-59],\n",
      "       [(1, 6), 2664.1260552106023, 0.0],\n",
      "       [(1, 7), 10.046099626690356, 0.0015267084264572986],\n",
      "       [(1, 8), 32.93343783086134, 9.536875298391631e-09],\n",
      "       [(1, 9), 1.5479126024251095, 0.2134438620154507],\n",
      "       [(1, 10), 39.41603704476013, 3.4247273514759863e-10],\n",
      "       [(2, 3), 15.83565953507465, 6.908812885376528e-05],\n",
      "       [(2, 4), 0.009721311293843723, 0.9214584820474485],\n",
      "       [(2, 5), 4.647105911570655, 0.03110594881998194],\n",
      "       [(2, 6), 235.67185793236314, 3.455165357005553e-53],\n",
      "       [(2, 7), 5.620137944965293, 0.017755251337434344],\n",
      "       [(2, 8), 18.270372960495067, 1.916650128116844e-05],\n",
      "       [(2, 9), 7.239129818762422, 0.007133154294051482],\n",
      "       [(2, 10), 118.8951463405916, 1.1041448562677791e-27],\n",
      "       [(3, 4), 0.9117685382522298, 0.3396456085601519],\n",
      "       [(3, 5), 219.6902739350713, 1.0566787042141396e-49],\n",
      "       [(3, 6), 5208.401391201502, 0.0],\n",
      "       [(3, 7), 0.008918498852263276, 0.9247613866134996],\n",
      "       [(3, 8), 0.046478597706333724, 0.8293081656462173],\n",
      "       [(3, 9), 6.920026454993092, 0.008523571508087965],\n",
      "       [(3, 10), 74.51841132491028, 6.007537904231213e-18],\n",
      "       [(4, 5), 33.310190802979996, 7.856982559786644e-09],\n",
      "       [(4, 6), 383.65474976475457, 1.991932753138197e-85],\n",
      "       [(4, 7), 0.4877362140657395, 0.4849388690367523],\n",
      "       [(4, 8), 1.4912378282494028, 0.2220245030965399],\n",
      "       [(4, 9), 0.12070255102174249, 0.7282737619947819],\n",
      "       [(4, 10), 0.3345284172274726, 0.5630046803352262],\n",
      "       [(5, 6), 65356.13267490786, 0.0],\n",
      "       [(5, 7), 42.76774808448627, 6.16400890646126e-11],\n",
      "       [(5, 8), 118.37904486079618, 1.4322717315791698e-27],\n",
      "       [(5, 9), 1.198205923095046, 0.2736805515136965],\n",
      "       [(5, 10), 18.843710105357573, 1.4187818123128376e-05],\n",
      "       [(6, 7), 2006.0978531970723, 0.0],\n",
      "       [(6, 8), 6396.761132225498, 0.0],\n",
      "       [(6, 9), 706.070377763044, 1.4310957529236272e-155],\n",
      "       [(6, 10), 10216.95749072478, 0.0],\n",
      "       [(7, 8), 0.0011206675913004406, 0.9732947063829732],\n",
      "       [(7, 9), 3.27163766490167, 0.07048722607006397],\n",
      "       [(7, 10), 32.300733412489635, 1.3206347950272917e-08],\n",
      "       [(8, 9), 11.17898405917531, 0.0008272905925143494],\n",
      "       [(8, 10), 108.21065586279002, 2.4167683270164426e-25],\n",
      "       [(9, 10), 111.37317427255647, 4.901812380483663e-26],\n",
      "       [(0, 1, 2), 1.0165461754723766, 0.313339669262421],\n",
      "       [(0, 1, 3), 144.8198583561815, 2.3514976685027836e-33],\n",
      "       [(0, 1, 4), 12.861787816909597, 0.00033536004762442045],\n",
      "       [(0, 1, 5), 1996.857997983901, 0.0],\n",
      "       [(0, 1, 6), 21176.01987626875, 0.0],\n",
      "       [(0, 1, 7), 61.1745228585682, 5.223309418075136e-15],\n",
      "       [(0, 1, 8), 203.6909632949393, 3.2690150941552694e-46],\n",
      "       [(0, 1, 9), 6.042808423278137, 0.013963054745042357],\n",
      "       [(0, 1, 10), 203.1321683219147, 4.3286105665192276e-46],\n",
      "       [(0, 2, 3), 198.32478036094366, 4.846294805151409e-45],\n",
      "       [(0, 2, 4), 0.6517634315121701, 0.4194829115128528],\n",
      "       [(0, 2, 5), 112.28962079906196, 3.08747770417033e-26],\n",
      "       [(0, 2, 6), 1326.4081752949107, 2.062524038538563e-290],\n",
      "       [(0, 2, 7), 74.52774120717945, 5.979213495776032e-18],\n",
      "       [(0, 2, 8), 239.52620763796136, 4.989003676955181e-54],\n",
      "       [(0, 2, 9), 90.44541133650571, 1.9015140520085942e-21],\n",
      "       [(0, 2, 10), 1376.5573187736254, 2.6097818463884404e-301],\n",
      "       [(0, 3, 4), 2.406873821282895, 0.12080339596927461],\n",
      "       [(0, 3, 5), 1343.5006197794692, 3.981490349945153e-294],\n",
      "       [(0, 3, 6), 38230.09072332841, 0.0],\n",
      "       [(0, 3, 7), 12.702846953103752, 0.0003650993063515377],\n",
      "       [(0, 3, 8), 37.608403512066516, 8.647114092047888e-10],\n",
      "       [(0, 3, 9), 123.38857165159514, 1.1464723375351022e-28],\n",
      "       [(0, 3, 10), 1334.398937246338, 3.7837707246820944e-292],\n",
      "       [(0, 4, 5), 232.2825889245142, 1.8947785215996555e-52],\n",
      "       [(0, 4, 6), 2897.9060512034353, 0.0],\n",
      "       [(0, 4, 7), 1.723122724196807, 0.18929157756539847],\n",
      "       [(0, 4, 8), 5.248639215454939, 0.021963941097092973],\n",
      "       [(0, 4, 9), 0.15506021640357323, 0.6937457891521469],\n",
      "       [(0, 4, 10), 0.2358781432564082, 0.6271991171380922],\n",
      "       [(0, 5, 6), 535905.2802762019, 0.0],\n",
      "       [(0, 5, 7), 224.85845943665737, 7.882808550265159e-51],\n",
      "       [(0, 5, 8), 637.1579232975876, 1.3869233657353813e-140],\n",
      "       [(0, 5, 9), 46.37726370052543, 9.754079835636601e-12],\n",
      "       [(0, 5, 10), 451.92026273189526, 2.755594016435316e-100],\n",
      "       [(0, 6, 7), 14957.307607496474, 0.0],\n",
      "       [(0, 6, 8), 48262.93515852798, 0.0],\n",
      "       [(0, 6, 9), 4845.442974800315, 0.0],\n",
      "       [(0, 6, 10), 75911.31467697705, 0.0],\n",
      "       [(0, 7, 8), 15.091259086566367, 0.00010243651333834554],\n",
      "       [(0, 7, 9), 53.21113189993864, 2.995564049756552e-13],\n",
      "       [(0, 7, 10), 538.3382642261749, 4.334516316115705e-119],\n",
      "       [(0, 8, 9), 174.89725756043828, 6.304389307780996e-40],\n",
      "       [(0, 8, 10), 1738.3281666782382, 0.0],\n",
      "       [(0, 9, 10), 1336.2804079050688, 1.475921128763483e-292],\n",
      "       [(1, 2, 3), 0.05816886414356999, 0.8094138891509818],\n",
      "       [(1, 2, 4), 0.16146459766939447, 0.687811661635108],\n",
      "       [(1, 2, 5), 28.450307714806456, 9.613448590139239e-08],\n",
      "       [(1, 2, 6), 514.8880144738779, 5.4793937653937555e-114],\n",
      "       [(1, 2, 7), 0.0013495249759854198, 0.9706955987897422],\n",
      "       [(1, 2, 8), 0.004603731871654974, 0.9459043995942447],\n",
      "       [(1, 2, 9), 0.2164244328981841, 0.6417779314575169],\n",
      "       [(1, 2, 10), 4.270894113762592, 0.03877052483809881],\n",
      "       [(1, 3, 4), 4.450656067897219, 0.03488784114846018],\n",
      "       [(1, 3, 5), 943.9781914469719, 2.7023427617333585e-207],\n",
      "       [(1, 3, 6), 7365.604772136303, 0.0],\n",
      "       [(1, 3, 7), 25.60206919810725, 4.195892143105666e-07],\n",
      "       [(1, 3, 8), 85.43637738120249, 2.3927789454482618e-20],\n",
      "       [(1, 3, 9), 4.435243113146941, 0.03520420565464857],\n",
      "       [(1, 3, 10), 104.84486537185849, 1.3208253401759984e-24],\n",
      "       [(1, 4, 5), 54.17567626989266, 1.8334239871719153e-13],\n",
      "       [(1, 4, 6), 381.7069858110855, 5.288417997389357e-85],\n",
      "       [(1, 4, 7), 1.9498180298852152, 0.16260646108782462],\n",
      "       [(1, 4, 8), 6.269162077143316, 0.012285722922012333],\n",
      "       [(1, 4, 9), 0.9303846251082017, 0.33476300042935203],\n",
      "       [(1, 4, 10), 11.264123546137501, 0.0007901948074185792],\n",
      "       [(1, 5, 6), 77809.5411499349, 0.0],\n",
      "       [(1, 5, 7), 262.8167043544179, 4.174018046051465e-59],\n",
      "       [(1, 5, 8), 823.3852078162454, 4.4441279641714474e-181],\n",
      "       [(1, 5, 9), 70.57932634720457, 4.421260780538185e-17],\n",
      "       [(1, 5, 10), 1190.0953750791523, 8.666781534808132e-261],\n",
      "       [(1, 6, 7), 2663.222765612888, 0.0],\n",
      "       [(1, 6, 8), 8566.471228680413, 0.0],\n",
      "       [(1, 6, 9), 1182.849002661017, 3.256204186538451e-259],\n",
      "       [(1, 6, 10), 18610.137685730566, 0.0],\n",
      "       [(1, 7, 8), 32.99335227465956, 9.247452584401081e-09],\n",
      "       [(1, 7, 9), 1.5591804905474886, 0.2117852525531774],\n",
      "       [(1, 7, 10), 39.64169441555653, 3.0509949809341926e-10],\n",
      "       [(1, 8, 9), 4.899098072298738, 0.02687072623495549],\n",
      "       [(1, 8, 10), 128.63801657294658, 8.138645703144152e-30],\n",
      "       [(1, 9, 10), 0.5994424233040294, 0.43879084595727136],\n",
      "       [(2, 3, 4), 0.11570652961179284, 0.7337385807140028],\n",
      "       [(2, 3, 5), 12.512731852001771, 0.00040418816985133616],\n",
      "       [(2, 3, 6), 305.8558733275843, 1.7458789096293667e-68],\n",
      "       [(2, 3, 7), 15.7079114482159, 7.391436327745269e-05],\n",
      "       [(2, 3, 8), 51.12066559446015, 8.685903262755025e-13],\n",
      "       [(2, 3, 9), 18.340075018533273, 1.8477906327248687e-05],\n",
      "       [(2, 3, 10), 327.5532230416164, 3.278721816183098e-73],\n",
      "       [(2, 4, 5), 6.348046843121063, 0.01175106044892754],\n",
      "       [(2, 4, 6), 96.71065998504017, 8.023609936536603e-23],\n",
      "       [(2, 4, 7), 0.009365805497485856, 0.922903416619317],\n",
      "       [(2, 4, 8), 0.05205183839212638, 0.8195306256772273],\n",
      "       [(2, 4, 9), 0.0013171403665861265, 0.9710491871442426],\n",
      "       [(2, 4, 10), 1.7863361682026944, 0.18137320237301216],\n",
      "       [(2, 5, 6), 11029.881673853748, 0.0],\n",
      "       [(2, 5, 7), 4.488917285012775, 0.034115277650928234],\n",
      "       [(2, 5, 8), 16.79648404882524, 4.161032741132272e-05],\n",
      "       [(2, 5, 9), 16.318172600068152, 5.354796192982953e-05],\n",
      "       [(2, 5, 10), 416.8224598438504, 1.1997248178332773e-92],\n",
      "       [(2, 6, 7), 236.7398663828874, 2.0210639462405461e-53],\n",
      "       [(2, 6, 8), 770.8832077734122, 1.1553738869180277e-169],\n",
      "       [(2, 6, 9), 84.59108821233197, 3.669153274330998e-20],\n",
      "       [(2, 6, 10), 633.9034557708451, 7.077169891643277e-140],\n",
      "       [(2, 7, 8), 18.118116479815967, 2.0761668222267965e-05],\n",
      "       [(2, 7, 9), 7.191627152935576, 0.007324453243607563],\n",
      "       [(2, 7, 10), 118.04839417469209, 1.6920874801983996e-27],\n",
      "       [(2, 8, 9), 23.86118250180798, 1.035386544362516e-06],\n",
      "       [(2, 8, 10), 384.77795236371014, 1.1343388058204639e-85],\n",
      "       [(2, 9, 10), 128.27357076778821, 9.77906607676535e-30],\n",
      "       [(3, 4, 5), 114.44306122003486, 1.042161482898458e-26],\n",
      "       [(3, 4, 6), 972.0497298928481, 2.136649407080147e-213],\n",
      "       [(3, 4, 7), 0.9173433865505224, 0.3381734803826394],\n",
      "       [(3, 4, 8), 2.857631101764537, 0.0909413365194818],\n",
      "       [(3, 4, 9), 0.16540504896546354, 0.6842281969448758],\n",
      "       [(3, 4, 10), 0.23335326739578494, 0.6290485017901375],\n",
      "       [(3, 5, 6), 158042.60471296532, 0.0],\n",
      "       [(3, 5, 7), 222.1574471846326, 3.0605504368257658e-50],\n",
      "       [(3, 5, 8), 668.7537584032747, 1.864774971623413e-147],\n",
      "       [(3, 5, 9), 11.845696127466356, 0.0005779476611772011],\n",
      "       [(3, 5, 10), 67.69923904219607, 1.9043702181237454e-16],\n",
      "       [(3, 6, 7), 5220.27703736607, 0.0],\n",
      "       [(3, 6, 8), 17120.2748896206, 0.0],\n",
      "       [(3, 6, 9), 2291.995567633805, 0.0],\n",
      "       [(3, 6, 10), 26819.74216845237, 0.0],\n",
      "       [(3, 7, 8), 0.061956827980690916, 0.803429405535],\n",
      "       [(3, 7, 9), 6.820483929932891, 0.00901181453343112],\n",
      "       [(3, 7, 10), 73.16333301039336, 1.1935257868453089e-17],\n",
      "       [(3, 8, 9), 22.856405498196683, 1.7456658210540217e-06],\n",
      "       [(3, 8, 10), 237.4314075953036, 1.4281888486112953e-53],\n",
      "       [(3, 9, 10), 247.00237491413492, 1.1694037064579227e-55],\n",
      "       [(4, 5, 6), 12244.956587539571, 0.0],\n",
      "       [(4, 5, 7), 33.41231836516122, 7.45502741152089e-09],\n",
      "       [(4, 5, 8), 99.698966277095, 1.7741380233879644e-23],\n",
      "       [(4, 5, 9), 17.69118114346209, 2.5982842758263823e-05],\n",
      "       [(4, 5, 10), 128.39900538259192, 9.180166290595083e-30],\n",
      "       [(4, 6, 7), 383.5067510296844, 2.1453370908982477e-85],\n",
      "       [(4, 6, 8), 1209.7116249844898, 4.728236097132135e-265],\n",
      "       [(4, 6, 9), 234.514347548993, 6.178446189357628e-53],\n",
      "       [(4, 6, 10), 2586.1179867245064, 0.0],\n",
      "       [(4, 7, 8), 1.4997678234195158, 0.22070708953278179],\n",
      "       [(4, 7, 9), 0.1221150517379446, 0.7267517710615996],\n",
      "       [(4, 7, 10), 0.345938448253553, 0.5564213300594494],\n",
      "       [(4, 8, 9), 0.2896026185760668, 0.590475294200187],\n",
      "       [(4, 8, 10), 0.8505880168381105, 0.35638604388105777],\n",
      "       [(4, 9, 10), 0.09318009438034984, 0.7601725955233714],\n",
      "       [(5, 6, 7), 65502.41637514111, 0.0],\n",
      "       [(5, 6, 8), 207855.31352316248, 0.0],\n",
      "       [(5, 6, 9), 29310.522423097656, 0.0],\n",
      "       [(5, 6, 10), 368196.8629382845, 0.0],\n",
      "       [(5, 7, 8), 120.78179483259498, 4.265615197209828e-28],\n",
      "       [(5, 7, 9), 1.0715428440066461, 0.3005972137867501],\n",
      "       [(5, 7, 10), 16.706938161394845, 4.36211755259846e-05],\n",
      "       [(5, 8, 9), 8.725014500884047, 0.00313873573676815],\n",
      "       [(5, 8, 10), 117.1806944896359, 2.6206997151007648e-27],\n",
      "       [(5, 9, 10), 581.3590008449306, 1.8987639496920236e-128],\n",
      "       [(6, 7, 8), 6409.953334851403, 0.0],\n",
      "       [(6, 7, 9), 708.766863952862, 3.70944360912067e-156],\n",
      "       [(6, 7, 10), 10267.527333028109, 0.0],\n",
      "       [(6, 8, 9), 2193.126023777297, 0.0],\n",
      "       [(6, 8, 10), 32190.415345713256, 0.0],\n",
      "       [(6, 9, 10), 2552.137182999203, 0.0],\n",
      "       [(7, 8, 9), 11.021700657637481, 0.0009005141540782257],\n",
      "       [(7, 8, 10), 106.16744262460159, 6.776081331527599e-25],\n",
      "       [(7, 9, 10), 110.26405447209054, 8.576986887236993e-26],\n",
      "       [(8, 9, 10), 372.50162601168574, 5.3397075706141905e-83]],\n",
      "      dtype=object)), 0.6504271159874608)\n",
      "best acc for below:  0.6504271159874608\n",
      "number of features:  231\n",
      "p value:  1\n",
      "3-way interaction\n",
      "features chi/pval:  [[0 5.265256679580366 0.02175521435486419]\n",
      " [1 10.029710173863258 0.0015403527221121511]\n",
      " [2 5.666702872536871 0.017289923715079227]\n",
      " [3 0.0058402262756335616 0.9390838660041609]\n",
      " [4 0.4851959137615238 0.48607815988089265]\n",
      " [5 42.00590680180157 9.099812365887488e-11]\n",
      " [6 2002.3058996858863 0.0]\n",
      " [7 0.00014457202542018628 0.9904066181813496]\n",
      " [8 0.00012255107490063595 0.9911673757910429]\n",
      " [9 3.3176173522259527 0.0685410969134076]\n",
      " [10 32.90863266810389 9.659338720612162e-09]\n",
      " [(0, 1) 61.087791239690276 5.458559233735258e-15]\n",
      " [(0, 2) 75.00723709398163 4.6899177573750446e-18]\n",
      " [(0, 3) 13.043360434026635 0.0003043614910347032]\n",
      " [(0, 4) 1.7102788702259173 0.19094919251302145]\n",
      " [(0, 5) 220.4664333194569 7.155550807351051e-50]\n",
      " [(0, 6) 14938.361810841634 0.0]\n",
      " [(0, 7) 5.119573286175557 0.02365743331034771]\n",
      " [(0, 8) 15.554168634264991 8.017467061618895e-05]\n",
      " [(0, 9) 53.702478857220626 2.332687545670866e-13]\n",
      " [(0, 10) 544.9296772710032 1.59588116590238e-120]\n",
      " [(1, 2) 0.0011169711576811876 0.9733387689898608]\n",
      " [(1, 3) 25.565192194059833 4.2768532524227246e-07]\n",
      " [(1, 4) 1.951076807143802 0.16247086174795994]\n",
      " [(1, 5) 262.17641079016204 5.755963324417165e-59]\n",
      " [(1, 6) 2664.1260552106023 0.0]\n",
      " [(1, 7) 10.046099626690356 0.0015267084264572986]\n",
      " [(1, 8) 32.93343783086134 9.536875298391631e-09]\n",
      " [(1, 9) 1.5479126024251095 0.2134438620154507]\n",
      " [(1, 10) 39.41603704476013 3.4247273514759863e-10]\n",
      " [(2, 3) 15.83565953507465 6.908812885376528e-05]\n",
      " [(2, 4) 0.009721311293843723 0.9214584820474485]\n",
      " [(2, 5) 4.647105911570655 0.03110594881998194]\n",
      " [(2, 6) 235.67185793236314 3.455165357005553e-53]\n",
      " [(2, 7) 5.620137944965293 0.017755251337434344]\n",
      " [(2, 8) 18.270372960495067 1.916650128116844e-05]\n",
      " [(2, 9) 7.239129818762422 0.007133154294051482]\n",
      " [(2, 10) 118.8951463405916 1.1041448562677791e-27]\n",
      " [(3, 4) 0.9117685382522298 0.3396456085601519]\n",
      " [(3, 5) 219.6902739350713 1.0566787042141396e-49]\n",
      " [(3, 6) 5208.401391201502 0.0]\n",
      " [(3, 7) 0.008918498852263276 0.9247613866134996]\n",
      " [(3, 8) 0.046478597706333724 0.8293081656462173]\n",
      " [(3, 9) 6.920026454993092 0.008523571508087965]\n",
      " [(3, 10) 74.51841132491028 6.007537904231213e-18]\n",
      " [(4, 5) 33.310190802979996 7.856982559786644e-09]\n",
      " [(4, 6) 383.65474976475457 1.991932753138197e-85]\n",
      " [(4, 7) 0.4877362140657395 0.4849388690367523]\n",
      " [(4, 8) 1.4912378282494028 0.2220245030965399]\n",
      " [(4, 9) 0.12070255102174249 0.7282737619947819]\n",
      " [(4, 10) 0.3345284172274726 0.5630046803352262]\n",
      " [(5, 6) 65356.13267490786 0.0]\n",
      " [(5, 7) 42.76774808448627 6.16400890646126e-11]\n",
      " [(5, 8) 118.37904486079618 1.4322717315791698e-27]\n",
      " [(5, 9) 1.198205923095046 0.2736805515136965]\n",
      " [(5, 10) 18.843710105357573 1.4187818123128376e-05]\n",
      " [(6, 7) 2006.0978531970723 0.0]\n",
      " [(6, 8) 6396.761132225498 0.0]\n",
      " [(6, 9) 706.070377763044 1.4310957529236272e-155]\n",
      " [(6, 10) 10216.95749072478 0.0]\n",
      " [(7, 8) 0.0011206675913004406 0.9732947063829732]\n",
      " [(7, 9) 3.27163766490167 0.07048722607006397]\n",
      " [(7, 10) 32.300733412489635 1.3206347950272917e-08]\n",
      " [(8, 9) 11.17898405917531 0.0008272905925143494]\n",
      " [(8, 10) 108.21065586279002 2.4167683270164426e-25]\n",
      " [(9, 10) 111.37317427255647 4.901812380483663e-26]\n",
      " [(0, 1, 2) 1.0165461754723766 0.313339669262421]\n",
      " [(0, 1, 3) 144.8198583561815 2.3514976685027836e-33]\n",
      " [(0, 1, 4) 12.861787816909597 0.00033536004762442045]\n",
      " [(0, 1, 5) 1996.857997983901 0.0]\n",
      " [(0, 1, 6) 21176.01987626875 0.0]\n",
      " [(0, 1, 7) 61.1745228585682 5.223309418075136e-15]\n",
      " [(0, 1, 8) 203.6909632949393 3.2690150941552694e-46]\n",
      " [(0, 1, 9) 6.042808423278137 0.013963054745042357]\n",
      " [(0, 1, 10) 203.1321683219147 4.3286105665192276e-46]\n",
      " [(0, 2, 3) 198.32478036094366 4.846294805151409e-45]\n",
      " [(0, 2, 4) 0.6517634315121701 0.4194829115128528]\n",
      " [(0, 2, 5) 112.28962079906196 3.08747770417033e-26]\n",
      " [(0, 2, 6) 1326.4081752949107 2.062524038538563e-290]\n",
      " [(0, 2, 7) 74.52774120717945 5.979213495776032e-18]\n",
      " [(0, 2, 8) 239.52620763796136 4.989003676955181e-54]\n",
      " [(0, 2, 9) 90.44541133650571 1.9015140520085942e-21]\n",
      " [(0, 2, 10) 1376.5573187736254 2.6097818463884404e-301]\n",
      " [(0, 3, 4) 2.406873821282895 0.12080339596927461]\n",
      " [(0, 3, 5) 1343.5006197794692 3.981490349945153e-294]\n",
      " [(0, 3, 6) 38230.09072332841 0.0]\n",
      " [(0, 3, 7) 12.702846953103752 0.0003650993063515377]\n",
      " [(0, 3, 8) 37.608403512066516 8.647114092047888e-10]\n",
      " [(0, 3, 9) 123.38857165159514 1.1464723375351022e-28]\n",
      " [(0, 3, 10) 1334.398937246338 3.7837707246820944e-292]\n",
      " [(0, 4, 5) 232.2825889245142 1.8947785215996555e-52]\n",
      " [(0, 4, 6) 2897.9060512034353 0.0]\n",
      " [(0, 4, 7) 1.723122724196807 0.18929157756539847]\n",
      " [(0, 4, 8) 5.248639215454939 0.021963941097092973]\n",
      " [(0, 4, 9) 0.15506021640357323 0.6937457891521469]\n",
      " [(0, 4, 10) 0.2358781432564082 0.6271991171380922]\n",
      " [(0, 5, 6) 535905.2802762019 0.0]\n",
      " [(0, 5, 7) 224.85845943665737 7.882808550265159e-51]\n",
      " [(0, 5, 8) 637.1579232975876 1.3869233657353813e-140]\n",
      " [(0, 5, 9) 46.37726370052543 9.754079835636601e-12]\n",
      " [(0, 5, 10) 451.92026273189526 2.755594016435316e-100]\n",
      " [(0, 6, 7) 14957.307607496474 0.0]\n",
      " [(0, 6, 8) 48262.93515852798 0.0]\n",
      " [(0, 6, 9) 4845.442974800315 0.0]\n",
      " [(0, 6, 10) 75911.31467697705 0.0]\n",
      " [(0, 7, 8) 15.091259086566367 0.00010243651333834554]\n",
      " [(0, 7, 9) 53.21113189993864 2.995564049756552e-13]\n",
      " [(0, 7, 10) 538.3382642261749 4.334516316115705e-119]\n",
      " [(0, 8, 9) 174.89725756043828 6.304389307780996e-40]\n",
      " [(0, 8, 10) 1738.3281666782382 0.0]\n",
      " [(0, 9, 10) 1336.2804079050688 1.475921128763483e-292]\n",
      " [(1, 2, 3) 0.05816886414356999 0.8094138891509818]\n",
      " [(1, 2, 4) 0.16146459766939447 0.687811661635108]\n",
      " [(1, 2, 5) 28.450307714806456 9.613448590139239e-08]\n",
      " [(1, 2, 6) 514.8880144738779 5.4793937653937555e-114]\n",
      " [(1, 2, 7) 0.0013495249759854198 0.9706955987897422]\n",
      " [(1, 2, 8) 0.004603731871654974 0.9459043995942447]\n",
      " [(1, 2, 9) 0.2164244328981841 0.6417779314575169]\n",
      " [(1, 2, 10) 4.270894113762592 0.03877052483809881]\n",
      " [(1, 3, 4) 4.450656067897219 0.03488784114846018]\n",
      " [(1, 3, 5) 943.9781914469719 2.7023427617333585e-207]\n",
      " [(1, 3, 6) 7365.604772136303 0.0]\n",
      " [(1, 3, 7) 25.60206919810725 4.195892143105666e-07]\n",
      " [(1, 3, 8) 85.43637738120249 2.3927789454482618e-20]\n",
      " [(1, 3, 9) 4.435243113146941 0.03520420565464857]\n",
      " [(1, 3, 10) 104.84486537185849 1.3208253401759984e-24]\n",
      " [(1, 4, 5) 54.17567626989266 1.8334239871719153e-13]\n",
      " [(1, 4, 6) 381.7069858110855 5.288417997389357e-85]\n",
      " [(1, 4, 7) 1.9498180298852152 0.16260646108782462]\n",
      " [(1, 4, 8) 6.269162077143316 0.012285722922012333]\n",
      " [(1, 4, 9) 0.9303846251082017 0.33476300042935203]\n",
      " [(1, 4, 10) 11.264123546137501 0.0007901948074185792]\n",
      " [(1, 5, 6) 77809.5411499349 0.0]\n",
      " [(1, 5, 7) 262.8167043544179 4.174018046051465e-59]\n",
      " [(1, 5, 8) 823.3852078162454 4.4441279641714474e-181]\n",
      " [(1, 5, 9) 70.57932634720457 4.421260780538185e-17]\n",
      " [(1, 5, 10) 1190.0953750791523 8.666781534808132e-261]\n",
      " [(1, 6, 7) 2663.222765612888 0.0]\n",
      " [(1, 6, 8) 8566.471228680413 0.0]\n",
      " [(1, 6, 9) 1182.849002661017 3.256204186538451e-259]\n",
      " [(1, 6, 10) 18610.137685730566 0.0]\n",
      " [(1, 7, 8) 32.99335227465956 9.247452584401081e-09]\n",
      " [(1, 7, 9) 1.5591804905474886 0.2117852525531774]\n",
      " [(1, 7, 10) 39.64169441555653 3.0509949809341926e-10]\n",
      " [(1, 8, 9) 4.899098072298738 0.02687072623495549]\n",
      " [(1, 8, 10) 128.63801657294658 8.138645703144152e-30]\n",
      " [(1, 9, 10) 0.5994424233040294 0.43879084595727136]\n",
      " [(2, 3, 4) 0.11570652961179284 0.7337385807140028]\n",
      " [(2, 3, 5) 12.512731852001771 0.00040418816985133616]\n",
      " [(2, 3, 6) 305.8558733275843 1.7458789096293667e-68]\n",
      " [(2, 3, 7) 15.7079114482159 7.391436327745269e-05]\n",
      " [(2, 3, 8) 51.12066559446015 8.685903262755025e-13]\n",
      " [(2, 3, 9) 18.340075018533273 1.8477906327248687e-05]\n",
      " [(2, 3, 10) 327.5532230416164 3.278721816183098e-73]\n",
      " [(2, 4, 5) 6.348046843121063 0.01175106044892754]\n",
      " [(2, 4, 6) 96.71065998504017 8.023609936536603e-23]\n",
      " [(2, 4, 7) 0.009365805497485856 0.922903416619317]\n",
      " [(2, 4, 8) 0.05205183839212638 0.8195306256772273]\n",
      " [(2, 4, 9) 0.0013171403665861265 0.9710491871442426]\n",
      " [(2, 4, 10) 1.7863361682026944 0.18137320237301216]\n",
      " [(2, 5, 6) 11029.881673853748 0.0]\n",
      " [(2, 5, 7) 4.488917285012775 0.034115277650928234]\n",
      " [(2, 5, 8) 16.79648404882524 4.161032741132272e-05]\n",
      " [(2, 5, 9) 16.318172600068152 5.354796192982953e-05]\n",
      " [(2, 5, 10) 416.8224598438504 1.1997248178332773e-92]\n",
      " [(2, 6, 7) 236.7398663828874 2.0210639462405461e-53]\n",
      " [(2, 6, 8) 770.8832077734122 1.1553738869180277e-169]\n",
      " [(2, 6, 9) 84.59108821233197 3.669153274330998e-20]\n",
      " [(2, 6, 10) 633.9034557708451 7.077169891643277e-140]\n",
      " [(2, 7, 8) 18.118116479815967 2.0761668222267965e-05]\n",
      " [(2, 7, 9) 7.191627152935576 0.007324453243607563]\n",
      " [(2, 7, 10) 118.04839417469209 1.6920874801983996e-27]\n",
      " [(2, 8, 9) 23.86118250180798 1.035386544362516e-06]\n",
      " [(2, 8, 10) 384.77795236371014 1.1343388058204639e-85]\n",
      " [(2, 9, 10) 128.27357076778821 9.77906607676535e-30]\n",
      " [(3, 4, 5) 114.44306122003486 1.042161482898458e-26]\n",
      " [(3, 4, 6) 972.0497298928481 2.136649407080147e-213]\n",
      " [(3, 4, 7) 0.9173433865505224 0.3381734803826394]\n",
      " [(3, 4, 8) 2.857631101764537 0.0909413365194818]\n",
      " [(3, 4, 9) 0.16540504896546354 0.6842281969448758]\n",
      " [(3, 4, 10) 0.23335326739578494 0.6290485017901375]\n",
      " [(3, 5, 6) 158042.60471296532 0.0]\n",
      " [(3, 5, 7) 222.1574471846326 3.0605504368257658e-50]\n",
      " [(3, 5, 8) 668.7537584032747 1.864774971623413e-147]\n",
      " [(3, 5, 9) 11.845696127466356 0.0005779476611772011]\n",
      " [(3, 5, 10) 67.69923904219607 1.9043702181237454e-16]\n",
      " [(3, 6, 7) 5220.27703736607 0.0]\n",
      " [(3, 6, 8) 17120.2748896206 0.0]\n",
      " [(3, 6, 9) 2291.995567633805 0.0]\n",
      " [(3, 6, 10) 26819.74216845237 0.0]\n",
      " [(3, 7, 8) 0.061956827980690916 0.803429405535]\n",
      " [(3, 7, 9) 6.820483929932891 0.00901181453343112]\n",
      " [(3, 7, 10) 73.16333301039336 1.1935257868453089e-17]\n",
      " [(3, 8, 9) 22.856405498196683 1.7456658210540217e-06]\n",
      " [(3, 8, 10) 237.4314075953036 1.4281888486112953e-53]\n",
      " [(3, 9, 10) 247.00237491413492 1.1694037064579227e-55]\n",
      " [(4, 5, 6) 12244.956587539571 0.0]\n",
      " [(4, 5, 7) 33.41231836516122 7.45502741152089e-09]\n",
      " [(4, 5, 8) 99.698966277095 1.7741380233879644e-23]\n",
      " [(4, 5, 9) 17.69118114346209 2.5982842758263823e-05]\n",
      " [(4, 5, 10) 128.39900538259192 9.180166290595083e-30]\n",
      " [(4, 6, 7) 383.5067510296844 2.1453370908982477e-85]\n",
      " [(4, 6, 8) 1209.7116249844898 4.728236097132135e-265]\n",
      " [(4, 6, 9) 234.514347548993 6.178446189357628e-53]\n",
      " [(4, 6, 10) 2586.1179867245064 0.0]\n",
      " [(4, 7, 8) 1.4997678234195158 0.22070708953278179]\n",
      " [(4, 7, 9) 0.1221150517379446 0.7267517710615996]\n",
      " [(4, 7, 10) 0.345938448253553 0.5564213300594494]\n",
      " [(4, 8, 9) 0.2896026185760668 0.590475294200187]\n",
      " [(4, 8, 10) 0.8505880168381105 0.35638604388105777]\n",
      " [(4, 9, 10) 0.09318009438034984 0.7601725955233714]\n",
      " [(5, 6, 7) 65502.41637514111 0.0]\n",
      " [(5, 6, 8) 207855.31352316248 0.0]\n",
      " [(5, 6, 9) 29310.522423097656 0.0]\n",
      " [(5, 6, 10) 368196.8629382845 0.0]\n",
      " [(5, 7, 8) 120.78179483259498 4.265615197209828e-28]\n",
      " [(5, 7, 9) 1.0715428440066461 0.3005972137867501]\n",
      " [(5, 7, 10) 16.706938161394845 4.36211755259846e-05]\n",
      " [(5, 8, 9) 8.725014500884047 0.00313873573676815]\n",
      " [(5, 8, 10) 117.1806944896359 2.6206997151007648e-27]\n",
      " [(5, 9, 10) 581.3590008449306 1.8987639496920236e-128]\n",
      " [(6, 7, 8) 6409.953334851403 0.0]\n",
      " [(6, 7, 9) 708.766863952862 3.70944360912067e-156]\n",
      " [(6, 7, 10) 10267.527333028109 0.0]\n",
      " [(6, 8, 9) 2193.126023777297 0.0]\n",
      " [(6, 8, 10) 32190.415345713256 0.0]\n",
      " [(6, 9, 10) 2552.137182999203 0.0]\n",
      " [(7, 8, 9) 11.021700657637481 0.0009005141540782257]\n",
      " [(7, 8, 10) 106.16744262460159 6.776081331527599e-25]\n",
      " [(7, 9, 10) 110.26405447209054 8.576986887236993e-26]\n",
      " [(8, 9, 10) 372.50162601168574 5.3397075706141905e-83]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# essential imports\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from lda import evaluate_acc\n",
    "from lda import lda\n",
    "from lda import kfold_index\n",
    "from lda import standardize\n",
    "from lda import normalize\n",
    "\n",
    "# chi2 user\n",
    "from lda import chi2\n",
    "from logreg import LogReg\n",
    "\n",
    "# chi2 scikit\n",
    "# from sklearn.feature_selection import chi2\n",
    "\n",
    "# LDA test\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "dlist = []\n",
    "pvallist = [0.01, 0.05, 0.1, 0.25, 0.5, 1]\n",
    "interactionT = [1, 2, 3, 4, 5,6]\n",
    "\n",
    "# Load datasets\n",
    "# 1599 red win\n",
    "wine_path = \"./Data/winequality-red.csv\"\n",
    "data = np.genfromtxt(wine_path, delimiter=\";\", skip_header=1)\n",
    "\n",
    "for interaction in interactionT:\n",
    "    for pvali in pvallist:\n",
    "\n",
    "        X_wine = data[:, 0:11]  # select columns 1 through 11\n",
    "        y_wine = data[:, 11]\n",
    "\n",
    "        # convert y values to 0, 1\n",
    "        y_wine = np.where(y_wine < 5.5, 0, 1)\n",
    "\n",
    "        # standardize\n",
    "        # X_wine = standardize(X_wine)\n",
    "\n",
    "        # adding interaction terms\n",
    "        from itertools import combinations\n",
    "\n",
    "        list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "        comblist = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "        comb = combinations(comblist, 2)\n",
    "\n",
    "        i = 11\n",
    "        if interaction in [2, 3, 4, 5,6]:\n",
    "            for (a, b) in comb:\n",
    "                # print((a, b), i)\n",
    "                list.append((a, b))\n",
    "                i += 1\n",
    "                inter = X_wine[:, a] * X_wine[:, b]\n",
    "                X_wine = np.column_stack((X_wine, inter))\n",
    "            if interaction in [3, 4, 5,6]:\n",
    "                comb3 = combinations(comblist, 3)\n",
    "                for (a, b, c) in comb3:\n",
    "                    # print((a, b, c), i)\n",
    "                    list.append((a, b, c))\n",
    "                    i += 1\n",
    "                    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c]\n",
    "                    X_wine = np.column_stack((X_wine, inter))\n",
    "                if interaction in [4, 5,6]:\n",
    "                    comb4 = combinations(comblist, 4)\n",
    "                    for (a, b, c, d) in comb4:\n",
    "                        # print((a, b, c, d), i)\n",
    "                        list.append((a, b, c, d))\n",
    "                        i += 1\n",
    "                        inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]\n",
    "                        X_wine = np.column_stack((X_wine, inter))\n",
    "                    if interaction in [5,6]:\n",
    "                        comb5 = combinations(comblist, 5)\n",
    "                        for (a, b, c, d, e) in comb5:\n",
    "                            # print((a, b, c, d, e), i)\n",
    "                            list.append((a, b, c, d, e))\n",
    "                            i += 1\n",
    "                            inter = (\n",
    "                                X_wine[:, a]\n",
    "                                * X_wine[:, b]\n",
    "                                * X_wine[:, c]\n",
    "                                * X_wine[:, d]\n",
    "                                * X_wine[:, e]\n",
    "                            )\n",
    "                            X_wine = np.column_stack((X_wine, inter))\n",
    "                        if interaction == 6:\n",
    "                            comb6 = combinations(comblist, 6)\n",
    "                            for (a, b, c, d, e,f) in comb6:\n",
    "                                # print((a, b, c, d, e,f), i)\n",
    "                                list.append((a, b, c, d, e,f))\n",
    "                                i += 1\n",
    "                                inter = (\n",
    "                                    X_wine[:, a]\n",
    "                                    * X_wine[:, b]\n",
    "                                    * X_wine[:, c]\n",
    "                                    * X_wine[:, d]\n",
    "                                    * X_wine[:, e]\n",
    "                                    * X_wine[:, f]\n",
    "                                )\n",
    "                                X_wine = np.column_stack((X_wine, inter))\n",
    "                            \n",
    "        else:\n",
    "            print(\"interaction is 1-way: \", interaction)\n",
    "\n",
    "        # normalize\n",
    "        #X_wine = normalize(X_wine)\n",
    "\n",
    "        # chi squared table todo\n",
    "        chi, pval = chi2(X_wine, y_wine)\n",
    "        list = np.asarray(list)\n",
    "        chilist = np.column_stack((list, chi, pval))\n",
    "\n",
    "        # threshold of chisquare interaction\n",
    "        index_of_chi = np.where(pval > pvali)\n",
    "        # keep 1-way interaction terms when testing 2 way or larger\n",
    "        if interaction in [2, 3, 4, 5,6]:\n",
    "            index_of_chi = np.delete(index_of_chi, np.where(index_of_chi[0] < 11), axis=1)\n",
    "        # remove comment to delete insignificant interaction rows\n",
    "        X_wine = np.delete(X_wine, index_of_chi, axis=1)\n",
    "        chilist = np.delete(chilist, index_of_chi, axis=0)\n",
    "        #print(chilist)\n",
    "        print(\"number of features: \", len(chilist))\n",
    "        print(\"p value: \", pvali)\n",
    "        print(\"%d-way interaction\" % interaction)\n",
    "        y_wine = y_wine.reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "        # kfold test 1 times\n",
    "        score = []\n",
    "        for i in range(1):\n",
    "            for train_index, test_index in kfold_index(5, X_wine):\n",
    "                # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "                y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "                # project lda\n",
    "                ld = LogReg(learning_rate=0.001)\n",
    "                ld.fit(X_train, y_train)\n",
    "                y_pred = ld.predict(X_test)\n",
    "                score.append(evaluate_acc(y_test, y_pred))\n",
    "        print(\"mean acc: \", np.mean(score))\n",
    "        print(\"std acc: \", np.std(score))\n",
    "        print()\n",
    "        acc = np.mean(score)\n",
    "\n",
    "\n",
    "        dlist.append(((pvali,interaction,chilist),acc))\n",
    "\n",
    "\n",
    "\n",
    "#print(dlist)\n",
    "from operator import itemgetter\n",
    "print(max(dlist,key=itemgetter(1)))\n",
    "((pvali,interaction,chilist),acc)=max(dlist,key=itemgetter(1))\n",
    "print(\"best acc for below: \", acc)\n",
    "print(\"number of features: \", len(chilist))\n",
    "print(\"p value: \", pvali)\n",
    "print(\"%d-way interaction\" % interaction)\n",
    "print(\"features chi/pval: \", chilist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# individual test (normalized)\n",
    "## no interactions (chi = 0.0000001) (74%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 8.89104331e+00 2.86572935e-03]\n",
      " [2.00000000e+00 5.66670287e+00 1.72899237e-02]\n",
      " [6.00000000e+00 8.12431060e+00 4.36755887e-03]\n",
      " [9.00000000e+00 3.98440125e+00 4.59234196e-02]\n",
      " [1.00000000e+01 2.60853253e+01 3.26657278e-07]]\n",
      "number of features:  5\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "# 1599 red win\n",
    "wine_path = \"./Data/winequality-red.csv\"\n",
    "data = np.genfromtxt(wine_path, delimiter=\";\", skip_header=1)\n",
    "# with open(wine_path) as f:\n",
    "#     f.readline()\n",
    "#     data = np.loadtxt(f, delimiter=\";\")\n",
    "# f = open(wine_path)\n",
    "# f.readline()  # skip the header\n",
    "# data = np.loadtxt(f, delimiter=\";\")\n",
    "# f.close()\n",
    "\n",
    "X_wine = data[:, 0:11]  # select columns 1 through 11\n",
    "y_wine = data[:, 11]\n",
    "\n",
    "# convert y values to 0, 1\n",
    "y_wine = np.where(y_wine < 5.5, 0, 1)\n",
    "# print(y_wine[:100])\n",
    "# wine = list(zip(X_wine, y_wine))\n",
    "# print(\"Total number of wine\", len(wine))\n",
    "# print(X_wine[0])\n",
    "# print(y_wine[0])\n",
    "# print(wine[0])\n",
    "\n",
    "# standardize\n",
    "#X_wine = standardize(X_wine)\n",
    "\n",
    "# adding interaction terms\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "comblist = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "comb = combinations(comblist, 2)\n",
    "\n",
    "#i = 11\n",
    "#for (a, b) in comb:\n",
    "#    # print((a, b), i)\n",
    "#    list.append((a, b))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "#\n",
    "#comb3 = combinations(comblist, 3)\n",
    "#for (a, b, c) in comb3:\n",
    "#    #print((a, b, c), i)\n",
    "#    list.append((a, b, c))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "#\n",
    "#comb4 = combinations(comblist, 4)\n",
    "#for (a, b, c, d) in comb4:\n",
    "#    #print((a, b, c, d), i)\n",
    "#    list.append((a, b, c, d))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "#\n",
    "#comb5 = combinations(comblist, 5)\n",
    "#for (a, b, c, d, e) in comb5:\n",
    "#    #print((a, b, c, d, e), i)\n",
    "#    list.append((a, b, c, d, e))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]  * X_wine[:, e]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "    \n",
    "\n",
    "# normalize\n",
    "X_wine = normalize(X_wine)\n",
    "\n",
    "# chi squared table todo\n",
    "chi, pval = chi2(X_wine, y_wine)\n",
    "list = np.asarray(list)\n",
    "chilist = np.column_stack((list, chi, pval))\n",
    "\n",
    "# threshold of chisquare interaction\n",
    "index_of_chi = np.where(pval > 0.05)\n",
    "#keep no interaction terms\n",
    "#index_of_chi = np.delete(index_of_chi, np.where(index_of_chi[0] < 11), axis=1)\n",
    "# remove comment to delete insignificant interaction rows\n",
    "X_wine = np.delete(X_wine, index_of_chi , axis=1)\n",
    "chilist = np.delete(chilist, index_of_chi, axis=0)\n",
    "print(chilist)\n",
    "print(\"number of features: \",len(chilist))\n",
    "# variance threshold on features\n",
    "# print(X_wine.var(axis=0))\n",
    "# for lda removing features decreased accuracy\n",
    "# index_of_var = np.where(X_wine.var(axis=0) < 0.00000001)\n",
    "# X_wine = np.delete(X_wine, index_of_var[0] , axis=1)\n",
    "\n",
    "y_wine = y_wine.reshape(-1,1)\n",
    "# recombininng and shuffling\n",
    "#wine = np.column_stack((X_wine, y_wine))\n",
    "#np.random.shuffle(wine)\n",
    "\n",
    "# split into train test set\n",
    "#n = X_wine.shape[0]\n",
    "#f_size = X_wine.shape[1]\n",
    "#X_wine = wine[:, 0:f_size]\n",
    "#X_wine_test = wine[:, 0:f_size]\n",
    "#y_wine = wine[:, f_size].reshape(-1,1)\n",
    "#y_wine_test = wine[:, f_size].reshape(-1,1)\n",
    "\n",
    "# standardize\n",
    "\n",
    "#X_wine = normalize(X_wine)\n",
    "#X_wine_test= normalize(X_wine_test)\n",
    "\n",
    "\n",
    "# qualities = [float(item[-1]) for item in wines[1:]]\n",
    "# sum(qualities) / len(qualities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  5\n",
      "0.742024882445141\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 way interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1.0421852090088097 0.3073137941896506]\n",
      " [1 8.891043306316842 0.0028657293536039487]\n",
      " [2 5.666702872536871 0.017289923715079227]\n",
      " [3 0.0006196962034192414 0.980139753892595]\n",
      " [4 0.93881024263718 0.33258400473573235]\n",
      " [5 0.6314063119772613 0.42683993824974265]\n",
      " [6 8.124310600120374 0.004367558871853705]\n",
      " [7 1.5846430385807455 0.20809316689519622]\n",
      " [8 0.0005594550998740942 0.9811295530403128]\n",
      " [9 3.9844012485431906 0.04592341963360741]\n",
      " [10 26.08532534641509 3.266572782179879e-07]\n",
      " [(0, 1) 7.326107125644484 0.006796008852444809]\n",
      " [(0, 2) 6.326521347333127 0.011894540260780607]\n",
      " [(0, 6) 7.65742921374715 0.005653901097783527]\n",
      " [(0, 9) 5.930387759132255 0.01488196682923952]\n",
      " [(0, 10) 6.439397254546957 0.011161651229169178]\n",
      " [(1, 3) 3.5662748142960847 0.05896444875823862]\n",
      " [(1, 4) 6.56301660386709 0.010411971648554024]\n",
      " [(1, 5) 5.411228701708492 0.02000762852905959]\n",
      " [(1, 6) 15.512383994186933 8.196649778217143e-05]\n",
      " [(1, 7) 8.949767167109705 0.0027750495281751927]\n",
      " [(1, 8) 8.38800691726057 0.003777048435039474]\n",
      " [(1, 10) 3.378871147724717 0.06603727083576172]\n",
      " [(2, 7) 5.622386899725186 0.01773248295055001]\n",
      " [(2, 8) 6.668019328647809 0.009815821564078139]\n",
      " [(2, 9) 3.619564909381211 0.057103805366640836]\n",
      " [(2, 10) 11.576937326250393 0.0006677483024442494]\n",
      " [(3, 6) 2.332805523609757 0.1266733922128427]\n",
      " [(4, 6) 10.043763191404896 0.0015286460135101759]\n",
      " [(5, 6) 5.789898831998599 0.016118516748681977]\n",
      " [(6, 7) 8.193999786017809 0.004202915911067666]\n",
      " [(6, 8) 8.563313052113898 0.0034300465235166167]\n",
      " [(6, 9) 2.9353226414549694 0.0866613467932992]\n",
      " [(6, 10) 3.3657477009461934 0.06656536094270445]\n",
      " [(7, 9) 3.9304001575140095 0.0474206593168381]\n",
      " [(7, 10) 26.02309178189872 3.3735805335913554e-07]\n",
      " [(8, 9) 4.8618797990463865 0.027456405078769848]\n",
      " [(8, 10) 12.663637254914672 0.00037283618127853513]\n",
      " [(9, 10) 12.05482508856082 0.0005165849504811061]]\n",
      "number of features:  39\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "# 1599 red win\n",
    "wine_path = \"./Data/winequality-red.csv\"\n",
    "data = np.genfromtxt(wine_path, delimiter=\";\", skip_header=1)\n",
    "# with open(wine_path) as f:\n",
    "#     f.readline()\n",
    "#     data = np.loadtxt(f, delimiter=\";\")\n",
    "# f = open(wine_path)\n",
    "# f.readline()  # skip the header\n",
    "# data = np.loadtxt(f, delimiter=\";\")\n",
    "# f.close()\n",
    "\n",
    "X_wine = data[:, 0:11]  # select columns 1 through 11\n",
    "y_wine = data[:, 11]\n",
    "\n",
    "# convert y values to 0, 1\n",
    "y_wine = np.where(y_wine < 5.5, 0, 1)\n",
    "# print(y_wine[:100])\n",
    "# wine = list(zip(X_wine, y_wine))\n",
    "# print(\"Total number of wine\", len(wine))\n",
    "# print(X_wine[0])\n",
    "# print(y_wine[0])\n",
    "# print(wine[0])\n",
    "\n",
    "# standardize\n",
    "#X_wine = standardize(X_wine)\n",
    "\n",
    "# adding interaction terms\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "comblist = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "comb = combinations(comblist, 2)\n",
    "\n",
    "i = 11\n",
    "for (a, b) in comb:\n",
    "    # print((a, b), i)\n",
    "    list.append((a, b))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "#\n",
    "#comb3 = combinations(comblist, 3)\n",
    "#for (a, b, c) in comb3:\n",
    "#    #print((a, b, c), i)\n",
    "#    list.append((a, b, c))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "#\n",
    "#comb4 = combinations(comblist, 4)\n",
    "#for (a, b, c, d) in comb4:\n",
    "#    #print((a, b, c, d), i)\n",
    "#    list.append((a, b, c, d))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "#\n",
    "#comb5 = combinations(comblist, 5)\n",
    "#for (a, b, c, d, e) in comb5:\n",
    "#    #print((a, b, c, d, e), i)\n",
    "#    list.append((a, b, c, d, e))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]  * X_wine[:, e]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "    \n",
    "\n",
    "# normalize\n",
    "X_wine = normalize(X_wine)\n",
    "\n",
    "# chi squared table todo\n",
    "chi, pval = chi2(X_wine, y_wine)\n",
    "list = np.asarray(list)\n",
    "chilist = np.column_stack((list, chi, pval))\n",
    "\n",
    "# threshold of chisquare interaction\n",
    "index_of_chi = np.where(pval < 0.05)\n",
    "#keep no interaction terms\n",
    "index_of_chi = np.delete(index_of_chi, np.where(index_of_chi[0] < 11), axis=1)\n",
    "# remove comment to delete insignificant interaction rows\n",
    "#X_wine = np.delete(X_wine, index_of_chi , axis=1)\n",
    "chilist = np.delete(chilist, index_of_chi, axis=0)\n",
    "print(chilist)\n",
    "print(\"number of features: \",len(chilist))\n",
    "\n",
    "# variance threshold on features\n",
    "# print(X_wine.var(axis=0))\n",
    "# for lda removing features decreased accuracy\n",
    "# index_of_var = np.where(X_wine.var(axis=0) < 0.00000001)\n",
    "# X_wine = np.delete(X_wine, index_of_var[0] , axis=1)\n",
    "\n",
    "\n",
    "# recombininng and shuffling\n",
    "wine = np.column_stack((X_wine, y_wine))\n",
    "#np.random.shuffle(wine)\n",
    "\n",
    "# split into train test set\n",
    "n = X_wine.shape[0]\n",
    "f_size = X_wine.shape[1]\n",
    "X_wine = wine[(n // 10) :, 0:f_size]\n",
    "X_wine_test = wine[: (n // 10), 0:f_size]\n",
    "y_wine = wine[(n // 10) :, f_size].reshape(-1,1)\n",
    "y_wine_test = wine[: (n // 10), f_size].reshape(-1,1)\n",
    "\n",
    "# standardize\n",
    "\n",
    "#X_wine = normalize(X_wine)\n",
    "#X_wine_test= normalize(X_wine_test)\n",
    "\n",
    "\n",
    "# qualities = [float(item[-1]) for item in wines[1:]]\n",
    "# sum(qualities) / len(qualities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  39\n",
      "0.745138888888889\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  66\n",
      "0.7454166666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 way interaction (highest 74.7%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1.0421852090088097 0.3073137941896506]\n",
      " [1 8.891043306316842 0.0028657293536039487]\n",
      " [2 5.666702872536871 0.017289923715079227]\n",
      " [3 0.0006196962034192414 0.980139753892595]\n",
      " [4 0.93881024263718 0.33258400473573235]\n",
      " [5 0.6314063119772613 0.42683993824974265]\n",
      " [6 8.124310600120374 0.004367558871853705]\n",
      " [7 1.5846430385807455 0.20809316689519622]\n",
      " [8 0.0005594550998740942 0.9811295530403128]\n",
      " [9 3.9844012485431906 0.04592341963360741]\n",
      " [10 26.08532534641509 3.266572782179879e-07]\n",
      " [(0, 1) 7.326107125644484 0.006796008852444809]\n",
      " [(0, 2) 6.326521347333127 0.011894540260780607]\n",
      " [(0, 3) 0.11449164602399525 0.7350872857384305]\n",
      " [(0, 4) 0.3471371653723725 0.5557382022901459]\n",
      " [(0, 5) 0.37313859717896325 0.5412984146053782]\n",
      " [(0, 6) 7.65742921374715 0.005653901097783527]\n",
      " [(0, 7) 1.0095282109476136 0.3150158915915632]\n",
      " [(0, 8) 1.4343251181540113 0.23105987186601448]\n",
      " [(0, 9) 5.930387759132255 0.01488196682923952]\n",
      " [(0, 10) 6.439397254546957 0.011161651229169178]\n",
      " [(1, 2) 0.0016756242989517714 0.9673482010197911]\n",
      " [(1, 3) 3.5662748142960847 0.05896444875823862]\n",
      " [(1, 4) 6.56301660386709 0.010411971648554024]\n",
      " [(1, 5) 5.411228701708492 0.02000762852905959]\n",
      " [(1, 6) 15.512383994186933 8.196649778217143e-05]\n",
      " [(1, 7) 8.949767167109705 0.0027750495281751927]\n",
      " [(1, 8) 8.38800691726057 0.003777048435039474]\n",
      " [(1, 9) 1.9380322588037207 0.16388234773001703]\n",
      " [(1, 10) 3.378871147724717 0.06603727083576172]\n",
      " [(2, 3) 1.4605847200769861 0.2268371085934905]\n",
      " [(2, 4) 0.015936575891546456 0.899541829155706]\n",
      " [(2, 5) 0.14522205973658298 0.7031437965447894]\n",
      " [(2, 6) 1.1992258189108718 0.2734764670584695]\n",
      " [(2, 7) 5.622386899725186 0.01773248295055001]\n",
      " [(2, 8) 6.668019328647809 0.009815821564078139]\n",
      " [(2, 9) 3.619564909381211 0.057103805366640836]\n",
      " [(2, 10) 11.576937326250393 0.0006677483024442494]\n",
      " [(3, 4) 0.3199147600522678 0.5716588755381449]\n",
      " [(3, 5) 0.23808347132489152 0.625593772291988]\n",
      " [(3, 6) 2.332805523609757 0.1266733922128427]\n",
      " [(3, 7) 0.0009394982830846902 0.9755476807126076]\n",
      " [(3, 8) 0.0014528469887084112 0.9695950110520054]\n",
      " [(3, 9) 0.7910687870929674 0.3737771433180471]\n",
      " [(3, 10) 0.9563517439791573 0.3281077680810557]\n",
      " [(4, 5) 1.8150715251623142 0.17790104832915915]\n",
      " [(4, 6) 10.043763191404896 0.0015286460135101759]\n",
      " [(4, 7) 0.9445611538733882 0.33110758356078585]\n",
      " [(4, 8) 0.9428539750895213 0.3315449514998463]\n",
      " [(4, 9) 0.10764049547478022 0.742846555999139]\n",
      " [(4, 10) 0.0707324101116959 0.7902733620029275]\n",
      " [(5, 6) 5.789898831998599 0.016118516748681977]\n",
      " [(5, 7) 0.6446824867250649 0.4220202697554225]\n",
      " [(5, 8) 0.5340886451006032 0.4648929622459276]\n",
      " [(5, 9) 0.020687656903766973 0.8856330984444699]\n",
      " [(5, 10) 0.027314884617953183 0.8687297759283529]\n",
      " [(6, 7) 8.193999786017809 0.004202915911067666]\n",
      " [(6, 8) 8.563313052113898 0.0034300465235166167]\n",
      " [(6, 9) 2.9353226414549694 0.0866613467932992]\n",
      " [(6, 10) 3.3657477009461934 0.06656536094270445]\n",
      " [(7, 8) 0.005307708291867783 0.9419222830468627]\n",
      " [(7, 9) 3.9304001575140095 0.0474206593168381]\n",
      " [(7, 10) 26.02309178189872 3.3735805335913554e-07]\n",
      " [(8, 9) 4.8618797990463865 0.027456405078769848]\n",
      " [(8, 10) 12.663637254914672 0.00037283618127853513]\n",
      " [(9, 10) 12.054825088560865 0.0005165849504810938]\n",
      " [(0, 1, 2) 0.12516945135696414 0.7234940574374988]\n",
      " [(0, 1, 3) 1.9628484063393332 0.1612090138845128]\n",
      " [(0, 1, 4) 4.71707675094134 0.029864471942963808]\n",
      " [(0, 1, 5) 5.3201844894549986 0.02107977951070341]\n",
      " [(0, 1, 6) 12.65620877588664 0.0003743205007183088]\n",
      " [(0, 1, 7) 7.345817614863074 0.006721895361822696]\n",
      " [(0, 1, 8) 7.078459583153229 0.007801604811222406]\n",
      " [(0, 1, 9) 0.6762060485643804 0.4108962270486335]\n",
      " [(0, 1, 10) 2.220114671285012 0.1362229414364409]\n",
      " [(0, 2, 3) 2.558616743892202 0.10969452448317761]\n",
      " [(0, 2, 4) 0.11613746106774225 0.7332620801709118]\n",
      " [(0, 2, 5) 0.35963981705376646 0.5487063399445906]\n",
      " [(0, 2, 6) 0.8543647925130933 0.3553204845033]\n",
      " [(0, 2, 7) 6.266026751438208 0.012307482552397537]\n",
      " [(0, 2, 8) 6.848458554574695 0.008871778402327122]\n",
      " [(0, 2, 9) 4.915511485679664 0.026616581198853823]\n",
      " [(0, 2, 10) 8.939177284289219 0.002791184084767779]\n",
      " [(0, 3, 4) 0.08002512803865383 0.7772633609509468]\n",
      " [(0, 3, 5) 0.1622692689816743 0.6870757919174162]\n",
      " [(0, 3, 6) 2.2036939243832334 0.13768045955445404]\n",
      " [(0, 3, 7) 0.11096225433199738 0.7390512712760211]\n",
      " [(0, 3, 8) 0.10865976466637225 0.741675164049688]\n",
      " [(0, 3, 9) 1.346658863374383 0.24586306915194722]\n",
      " [(0, 3, 10) 1.1556159830865056 0.2823766060058023]\n",
      " [(0, 4, 5) 1.3839648598961871 0.23942694032244985]\n",
      " [(0, 4, 6) 8.180197163028609 0.004235015683255893]\n",
      " [(0, 4, 7) 0.3495564589741069 0.5543643153147575]\n",
      " [(0, 4, 8) 0.38974401079947774 0.5324339912065981]\n",
      " [(0, 4, 9) 0.014754433000463892 0.9033204963013431]\n",
      " [(0, 4, 10) 0.00519854800780622 0.9425215645924413]\n",
      " [(0, 5, 6) 6.397371192891337 0.011428947278196646]\n",
      " [(0, 5, 7) 0.38049024100935314 0.5373409843969472]\n",
      " [(0, 5, 8) 0.32679108996335043 0.5675547479536425]\n",
      " [(0, 5, 9) 0.08704757513205007 0.7679646522859412]\n",
      " [(0, 5, 10) 0.07100303304751349 0.7898819249714917]\n",
      " [(0, 6, 7) 7.717399173922734 0.005469112899213425]\n",
      " [(0, 6, 8) 8.301540196533232 0.003961147086714259]\n",
      " [(0, 6, 9) 2.321195335370562 0.1276219217165839]\n",
      " [(0, 6, 10) 3.209792164726842 0.07319878083675388]\n",
      " [(0, 7, 8) 1.3725526090735567 0.24137379390400415]\n",
      " [(0, 7, 9) 5.858555243955326 0.015501513115351143]\n",
      " [(0, 7, 10) 6.364071087261573 0.0116454043416054]\n",
      " [(0, 8, 9) 7.085453065283973 0.007771216964022094]\n",
      " [(0, 8, 10) 7.380715513434372 0.00659269361887852]\n",
      " [(0, 9, 10) 12.946068228611203 0.00032059410849762305]\n",
      " [(1, 2, 3) 0.016860540331469626 0.8966865724298432]\n",
      " [(1, 2, 4) 0.5090308879867432 0.47555867844149813]\n",
      " [(1, 2, 5) 1.7097540693994282 0.19101728184527206]\n",
      " [(1, 2, 6) 8.087889944234488 0.004456201431627697]\n",
      " [(1, 2, 7) 0.002037591627132324 0.963995965482171]\n",
      " [(1, 2, 8) 0.0018869640536410969 0.9653514304913271]\n",
      " [(1, 2, 9) 0.20810041624825015 0.6482603696946514]\n",
      " [(1, 2, 10) 0.5571288581592004 0.455419387873498]\n",
      " [(1, 3, 4) 2.6890816226600096 0.10103803320308499]\n",
      " [(1, 3, 5) 2.1086753234365037 0.14646605651979275]\n",
      " [(1, 3, 6) 9.794476869842013 0.0017503679872816153]\n",
      " [(1, 3, 7) 3.5538657735023125 0.0594068952974285]\n",
      " [(1, 3, 8) 3.676129302859862 0.05519684957168805]\n",
      " [(1, 3, 9) 0.7634535578992212 0.38224989685455346]\n",
      " [(1, 3, 10) 1.6613860792613226 0.19741628210519427]\n",
      " [(1, 4, 5) 5.47265968589771 0.019316207784072437]\n",
      " [(1, 4, 6) 18.34567729703906 1.8423651766229992e-05]\n",
      " [(1, 4, 7) 6.579988334790721 0.010313156841761381]\n",
      " [(1, 4, 8) 7.0292267540376905 0.008018997088568815]\n",
      " [(1, 4, 9) 1.5330524499879226 0.2156549039051614]\n",
      " [(1, 4, 10) 4.17172289097255 0.04110398158551982]\n",
      " [(1, 5, 6) 12.702507187052198 0.0003651656487460342]\n",
      " [(1, 5, 7) 5.428018812910896 0.019816148008396085]\n",
      " [(1, 5, 8) 4.900429205741495 0.026850021296118784]\n",
      " [(1, 5, 9) 2.210885069706707 0.13704002869787157]\n",
      " [(1, 5, 10) 2.481542888836197 0.11518920591548486]\n",
      " [(1, 6, 7) 15.539695190638396 8.079082182198608e-05]\n",
      " [(1, 6, 8) 15.769472029577381 7.15477107097734e-05]\n",
      " [(1, 6, 9) 9.84643534130892 0.0017016116879400996]\n",
      " [(1, 6, 10) 11.385322158631128 0.0007402671326848113]\n",
      " [(1, 7, 8) 8.444929817386278 0.0036606256418815247]\n",
      " [(1, 7, 9) 1.954214212419564 0.16213345207806862]\n",
      " [(1, 7, 10) 3.4149523812691154 0.06460827757603078]\n",
      " [(1, 8, 9) 1.9623426015751357 0.16126300289728254]\n",
      " [(1, 8, 10) 3.171207815331421 0.07494700269243096]\n",
      " [(1, 9, 10) 0.07804106464102539 0.7799698627239503]\n",
      " [(2, 3, 4) 0.05578906924387304 0.8132797310510249]\n",
      " [(2, 3, 5) 0.037779987475850624 0.8458855932320422]\n",
      " [(2, 3, 6) 0.18751325676872788 0.6649944216385961]\n",
      " [(2, 3, 7) 1.4534968098420697 0.2279676875603472]\n",
      " [(2, 3, 8) 1.3908727502534397 0.23825776830857287]\n",
      " [(2, 3, 9) 2.6970698556666526 0.10053285667676656]\n",
      " [(2, 3, 10) 2.8772869444366815 0.08983726829181023]\n",
      " [(2, 4, 5) 0.32520731778284084 0.5684949151193506]\n",
      " [(2, 4, 6) 2.2977110949166093 0.12956480479207005]\n",
      " [(2, 4, 7) 0.0153599234734646 0.9013666152044391]\n",
      " [(2, 4, 8) 0.031142657886876434 0.859922496302937]\n",
      " [(2, 4, 9) 0.0010796232513001546 0.9737881281539531]\n",
      " [(2, 4, 10) 0.3115340370077921 0.5767403610811495]\n",
      " [(2, 5, 6) 1.4966933542104317 0.22118084518262038]\n",
      " [(2, 5, 7) 0.1403347990762821 0.7079484033416061]\n",
      " [(2, 5, 8) 0.19100848629142253 0.6620785853261966]\n",
      " [(2, 5, 9) 0.2549714468760649 0.6135960797907902]\n",
      " [(2, 5, 10) 1.2521703311819539 0.26313836029843074]\n",
      " [(2, 6, 7) 1.212957049498061 0.27074727556591255]\n",
      " [(2, 6, 8) 1.303212792579942 0.25362720300734876]\n",
      " [(2, 6, 9) 0.612978900089358 0.43366840121626904]\n",
      " [(2, 6, 10) 0.2622474370182821 0.6085803227885784]\n",
      " [(2, 7, 8) 6.615097308929418 0.010111781390300886]\n",
      " [(2, 7, 9) 3.5972524774587598 0.057875147654030935]\n",
      " [(2, 7, 10) 11.473835331524176 0.0007058286640908018]\n",
      " [(2, 8, 9) 4.354230383541611 0.03691713619663243]\n",
      " [(2, 8, 10) 11.781826298852705 0.000598117760883485]\n",
      " [(2, 9, 10) 6.823062274882346 0.008998813637441807]\n",
      " [(3, 4, 5) 0.6589043924558685 0.41694700056968836]\n",
      " [(3, 4, 6) 3.2585107098385615 0.07105363117993664]\n",
      " [(3, 4, 7) 0.3203330399040363 0.5714075688950645]\n",
      " [(3, 4, 8) 0.31503960446876067 0.5746040225515485]\n",
      " [(3, 4, 9) 0.043098856345819544 0.835539242458528]\n",
      " [(3, 4, 10) 0.009304249448244532 0.9231564026390982]\n",
      " [(3, 5, 6) 1.0328509346886459 0.309490004464693]\n",
      " [(3, 5, 7) 0.24143117280757853 0.6231743519402217]\n",
      " [(3, 5, 8) 0.21761788613147093 0.6408609947445066]\n",
      " [(3, 5, 9) 0.01933059998681177 0.8894228645843208]\n",
      " [(3, 5, 10) 0.007097410213496087 0.9328607430617495]\n",
      " [(3, 6, 7) 2.354200794820298 0.12494590510712089]\n",
      " [(3, 6, 8) 2.5389651759624305 0.11106755807872803]\n",
      " [(3, 6, 9) 1.9929914895624012 0.15802844399919597]\n",
      " [(3, 6, 10) 0.9768937371130086 0.32296689752552404]\n",
      " [(3, 7, 8) 0.0019220056418031198 0.9650313964814707]\n",
      " [(3, 7, 9) 0.7747373297970344 0.3787554995118657]\n",
      " [(3, 7, 10) 0.9376032770891722 0.33289498060776823]\n",
      " [(3, 8, 9) 0.7888991991247932 0.3744331911684745]\n",
      " [(3, 8, 10) 0.8249253635158956 0.36374402967843966]\n",
      " [(3, 9, 10) 3.4073218345545415 0.0649077091599432]\n",
      " [(4, 5, 6) 6.960268214476985 0.008333953871305873]\n",
      " [(4, 5, 7) 1.8215751282972112 0.17712589926985584]\n",
      " [(4, 5, 8) 1.9888700996786342 0.15845907051478575]\n",
      " [(4, 5, 9) 0.48471126088848043 0.4862960233700917]\n",
      " [(4, 5, 10) 0.7449246355942303 0.388087894076274]\n",
      " [(4, 6, 7) 10.042781069069296 0.0015294612229701532]\n",
      " [(4, 6, 8) 11.616528218091837 0.0006536830519072723]\n",
      " [(4, 6, 9) 2.936291294438452 0.08660938164591561]\n",
      " [(4, 6, 10) 7.155602285547721 0.00747301965697979]\n",
      " [(4, 7, 8) 0.9502233351732083 0.32966245578517867]\n",
      " [(4, 7, 9) 0.10888143014299378 0.7414212228345136]\n",
      " [(4, 7, 10) 0.0731961954080434 0.7867387299275499]\n",
      " [(4, 8, 9) 0.09430550632204554 0.758773334564055]\n",
      " [(4, 8, 10) 0.05876142856413047 0.80846437154549]\n",
      " [(4, 9, 10) 0.008952510898208125 0.924618483019151]\n",
      " [(5, 6, 7) 5.8188843014456175 0.015854998990061858]\n",
      " [(5, 6, 8) 5.5236964840236205 0.018760575946221433]\n",
      " [(5, 6, 9) 4.010004280720779 0.0452310359379456]\n",
      " [(5, 6, 10) 2.819274911070661 0.09313846910548433]\n",
      " [(5, 7, 8) 0.5464766310812592 0.4597609252252176]\n",
      " [(5, 7, 9) 0.01851167673281906 0.8917757279984464]\n",
      " [(5, 7, 10) 0.024286341106441314 0.8761585447453712]\n",
      " [(5, 8, 9) 0.05531388400919085 0.8140620131286103]\n",
      " [(5, 8, 10) 0.05098205665002531 0.8213631488784726]\n",
      " [(5, 9, 10) 1.0655219095519606 0.3019591355712242]\n",
      " [(6, 7, 8) 8.640888866623008 0.003287010209313742]\n",
      " [(6, 7, 9) 2.953574907366243 0.0856878176066154]\n",
      " [(6, 7, 10) 3.4060148876376197 0.06495914365311113]\n",
      " [(6, 8, 9) 3.1084199822506378 0.07788848543113883]\n",
      " [(6, 8, 10) 3.49727219459665 0.06146999988493146]\n",
      " [(6, 9, 10) 1.0614150152431678 0.30289267456774066]\n",
      " [(7, 8, 9) 4.809010544375845 0.028311296466237035]\n",
      " [(7, 8, 10) 12.749873739690727 0.00035603319628734544]\n",
      " [(7, 9, 10) 11.972732535565001 0.0005398471772698047]\n",
      " [(8, 9, 10) 14.318768506303071 0.00015431859977841722]]\n",
      "231\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "# 1599 red win\n",
    "wine_path = \"./Data/winequality-red.csv\"\n",
    "data = np.genfromtxt(wine_path, delimiter=\";\", skip_header=1)\n",
    "# with open(wine_path) as f:\n",
    "#     f.readline()\n",
    "#     data = np.loadtxt(f, delimiter=\";\")\n",
    "# f = open(wine_path)\n",
    "# f.readline()  # skip the header\n",
    "# data = np.loadtxt(f, delimiter=\";\")\n",
    "# f.close()\n",
    "\n",
    "X_wine = data[:, 0:11]  # select columns 1 through 11\n",
    "y_wine = data[:, 11]\n",
    "\n",
    "# convert y values to 0, 1\n",
    "y_wine = np.where(y_wine < 5.5, 0, 1)\n",
    "# print(y_wine[:100])\n",
    "# wine = list(zip(X_wine, y_wine))\n",
    "# print(\"Total number of wine\", len(wine))\n",
    "# print(X_wine[0])\n",
    "# print(y_wine[0])\n",
    "# print(wine[0])\n",
    "\n",
    "# standardize\n",
    "#X_wine = standardize(X_wine)\n",
    "\n",
    "# adding interaction terms\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "comblist = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "comb = combinations(comblist, 2)\n",
    "\n",
    "i = 11\n",
    "for (a, b) in comb:\n",
    "    # print((a, b), i)\n",
    "    list.append((a, b))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "comb3 = combinations(comblist, 3)\n",
    "for (a, b, c) in comb3:\n",
    "    #print((a, b, c), i)\n",
    "    list.append((a, b, c))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "#comb4 = combinations(comblist, 4)\n",
    "#for (a, b, c, d) in comb4:\n",
    "#    #print((a, b, c, d), i)\n",
    "#    list.append((a, b, c, d))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "#\n",
    "#comb5 = combinations(comblist, 5)\n",
    "#for (a, b, c, d, e) in comb5:\n",
    "#    #print((a, b, c, d, e), i)\n",
    "#    list.append((a, b, c, d, e))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]  * X_wine[:, e]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "    \n",
    "\n",
    "# normalize\n",
    "X_wine = normalize(X_wine)\n",
    "\n",
    "# chi squared table todo\n",
    "chi, pval = chi2(X_wine, y_wine)\n",
    "list = np.asarray(list)\n",
    "chilist = np.column_stack((list, chi, pval))\n",
    "\n",
    "# threshold of chisquare interaction\n",
    "index_of_chi = np.where(chi < 0.0001)\n",
    "#keep no interaction terms\n",
    "index_of_chi = np.delete(index_of_chi, np.where(index_of_chi[0] < 11), axis=1)\n",
    "# remove comment to delete insignificant interaction rows\n",
    "#X_wine = np.delete(X_wine, index_of_chi , axis=1)\n",
    "chilist = np.delete(chilist, index_of_chi, axis=0)\n",
    "print(chilist)\n",
    "print(len(chilist))\n",
    "\n",
    "# variance threshold on features\n",
    "# print(X_wine.var(axis=0))\n",
    "# for lda removing features decreased accuracy\n",
    "# index_of_var = np.where(X_wine.var(axis=0) < 0.00000001)\n",
    "# X_wine = np.delete(X_wine, index_of_var[0] , axis=1)\n",
    "\n",
    "\n",
    "# recombininng and shuffling\n",
    "wine = np.column_stack((X_wine, y_wine))\n",
    "#np.random.shuffle(wine)\n",
    "\n",
    "# split into train test set\n",
    "n = X_wine.shape[0]\n",
    "f_size = X_wine.shape[1]\n",
    "X_wine = wine[(n // 10) :, 0:f_size]\n",
    "X_wine_test = wine[: (n // 10), 0:f_size]\n",
    "y_wine = wine[(n // 10) :, f_size].reshape(-1,1)\n",
    "y_wine_test = wine[: (n // 10), f_size].reshape(-1,1)\n",
    "\n",
    "# standardize\n",
    "\n",
    "#X_wine = normalize(X_wine)\n",
    "#X_wine_test= normalize(X_wine_test)\n",
    "\n",
    "\n",
    "# qualities = [float(item[-1]) for item in wines[1:]]\n",
    "# sum(qualities) / len(qualities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  231\n",
      "0.7468750000000002\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  165\n",
      "0.7460416666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  116\n",
      "0.747638888888889\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 way interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1.042185209008817 0.3073137941896489]\n",
      " [1 8.891043306316842 0.0028657293536039487]\n",
      " [2 5.666702872536845 0.01728992371507948]\n",
      " [3 0.0006196962034192906 0.9801397538925941]\n",
      " [4 0.93881024263718 0.33258400473573235]\n",
      " [5 0.6314063119772713 0.4268399382497391]\n",
      " [6 8.124310600120378 0.0043675588718536905]\n",
      " [7 1.5846430385807617 0.20809316689519386]\n",
      " [8 0.0005594550998740942 0.9811295530403128]\n",
      " [9 3.9844012485431595 0.045923419633608194]\n",
      " [10 26.08532534641502 3.2665727821800035e-07]\n",
      " [(0, 1) 7.326107125644496 0.0067960088524447555]\n",
      " [(0, 2) 6.326521347333114 0.01189454026078067]\n",
      " [(0, 6) 7.657429213747134 0.005653901097783591]\n",
      " [(0, 9) 5.930387759132255 0.01488196682923952]\n",
      " [(0, 10) 6.43939725454695 0.011161651229169232]\n",
      " [(1, 3) 3.5662748142960856 0.0589644487582384]\n",
      " [(1, 4) 6.563016603867104 0.010411971648553951]\n",
      " [(1, 5) 5.41122870170849 0.02000762852905962]\n",
      " [(1, 6) 15.512383994186958 8.196649778217025e-05]\n",
      " [(1, 7) 8.949767167109703 0.0027750495281751927]\n",
      " [(1, 8) 8.388006917260578 0.003777048435039452]\n",
      " [(1, 10) 3.3788711477247215 0.06603727083576152]\n",
      " [(2, 7) 5.622386899725173 0.017732482950550174]\n",
      " [(2, 8) 6.6680193286477945 0.009815821564078193]\n",
      " [(2, 9) 3.619564909381211 0.057103805366640836]\n",
      " [(2, 10) 11.576937326250402 0.0006677483024442473]\n",
      " [(4, 6) 10.043763191404892 0.001528646013510178]\n",
      " [(5, 6) 5.789898831998592 0.016118516748681994]\n",
      " [(6, 7) 8.19399978601782 0.004202915911067641]\n",
      " [(6, 8) 8.563313052113898 0.0034300465235166167]\n",
      " [(6, 10) 3.365747700946198 0.06656536094270422]\n",
      " [(7, 9) 3.930400157514023 0.04742065931683776]\n",
      " [(7, 10) 26.023091781898767 3.37358053359128e-07]\n",
      " [(8, 9) 4.861879799046382 0.027456405078770025]\n",
      " [(8, 10) 12.6636372549147 0.0003728361812785301]\n",
      " [(9, 10) 12.05482508856085 0.0005165849504810979]\n",
      " [(0, 1, 4) 4.717076750941336 0.02986447194296387]\n",
      " [(0, 1, 5) 5.32018448945499 0.021079779510703604]\n",
      " [(0, 1, 6) 12.65620877588664 0.0003743205007183088]\n",
      " [(0, 1, 7) 7.345817614863097 0.006721895361822601]\n",
      " [(0, 1, 8) 7.0784595831532275 0.0078016048112224115]\n",
      " [(0, 2, 7) 6.266026751438178 0.012307482552397738]\n",
      " [(0, 2, 8) 6.8484585545747025 0.00887177840232707]\n",
      " [(0, 2, 9) 4.915511485679668 0.02661658119885388]\n",
      " [(0, 2, 10) 8.939177284289215 0.0027911840847677856]\n",
      " [(0, 4, 6) 8.1801971630286 0.004235015683255917]\n",
      " [(0, 5, 6) 6.397371192891343 0.011428947278196603]\n",
      " [(0, 6, 7) 7.717399173922749 0.005469112899213384]\n",
      " [(0, 6, 8) 8.301540196533228 0.003961147086714267]\n",
      " [(0, 6, 10) 3.209792164726853 0.07319878083675337]\n",
      " [(0, 7, 9) 5.858555243955312 0.015501513115351246]\n",
      " [(0, 7, 10) 6.364071087261573 0.0116454043416054]\n",
      " [(0, 8, 9) 7.085453065283974 0.007771216964022078]\n",
      " [(0, 8, 10) 7.380715513434366 0.006592693618878551]\n",
      " [(0, 9, 10) 12.946068228611209 0.00032059410849762213]\n",
      " [(1, 2, 6) 8.087889944234474 0.0044562014316277335]\n",
      " [(1, 3, 6) 9.794476869842011 0.0017503679872816142]\n",
      " [(1, 3, 7) 3.553865773502304 0.059406895297429006]\n",
      " [(1, 3, 8) 3.6761293028598683 0.055196849571688085]\n",
      " [(1, 4, 5) 5.472659685897707 0.019316207784072448]\n",
      " [(1, 4, 6) 18.345677297039074 1.842365176622988e-05]\n",
      " [(1, 4, 7) 6.579988334790726 0.010313156841761352]\n",
      " [(1, 4, 8) 7.029226754037694 0.008018997088568786]\n",
      " [(1, 4, 10) 4.171722890972551 0.0411039815855198]\n",
      " [(1, 5, 6) 12.702507187052179 0.0003651656487460378]\n",
      " [(1, 5, 7) 5.428018812910889 0.019816148008396164]\n",
      " [(1, 5, 8) 4.900429205741495 0.026850021296118784]\n",
      " [(1, 6, 7) 15.539695190638414 8.079082182198535e-05]\n",
      " [(1, 6, 8) 15.769472029577393 7.154771070977296e-05]\n",
      " [(1, 6, 9) 9.846435341308908 0.0017016116879401116]\n",
      " [(1, 6, 10) 11.385322158631128 0.0007402671326848113]\n",
      " [(1, 7, 8) 8.44492981738627 0.003660625641881543]\n",
      " [(1, 7, 10) 3.414952381269126 0.06460827757603084]\n",
      " [(1, 8, 10) 3.1712078153314094 0.07494700269243125]\n",
      " [(2, 7, 8) 6.6150973089293785 0.01011178139030116]\n",
      " [(2, 7, 9) 3.5972524774587677 0.05787514765403059]\n",
      " [(2, 7, 10) 11.473835331524166 0.0007058286640908061]\n",
      " [(2, 8, 9) 4.354230383541616 0.03691713619663238]\n",
      " [(2, 8, 10) 11.781826298852685 0.0005981177608834917]\n",
      " [(2, 9, 10) 6.823062274882353 0.008998813637441781]\n",
      " [(3, 4, 6) 3.258510709838567 0.07105363117993688]\n",
      " [(3, 9, 10) 3.407321834554545 0.06490770915994291]\n",
      " [(4, 5, 6) 6.960268214476988 0.008333953871305876]\n",
      " [(4, 6, 7) 10.0427810690693 0.0015294612229701525]\n",
      " [(4, 6, 8) 11.616528218091851 0.000653683051907268]\n",
      " [(4, 6, 10) 7.155602285547733 0.007473019656979733]\n",
      " [(5, 6, 7) 5.818884301445614 0.015854998990061913]\n",
      " [(5, 6, 8) 5.523696484023612 0.01876057594622148]\n",
      " [(5, 6, 9) 4.010004280720782 0.04523103593794565]\n",
      " [(6, 7, 8) 8.640888866623008 0.003287010209313742]\n",
      " [(6, 7, 10) 3.4060148876376077 0.06495914365311171]\n",
      " [(6, 8, 9) 3.1084199822506413 0.07788848543113852]\n",
      " [(6, 8, 10) 3.497272194596657 0.06146999988493116]\n",
      " [(7, 8, 9) 4.809010544375845 0.028311296466237035]\n",
      " [(7, 8, 10) 12.749873739690702 0.00035603319628734994]\n",
      " [(7, 9, 10) 11.972732535565019 0.0005398471772698004]\n",
      " [(8, 9, 10) 14.318768506303071 0.00015431859977841722]\n",
      " [(0, 1, 2, 6) 5.8817415460464595 0.015298682288942421]\n",
      " [(0, 1, 3, 6) 7.440898089865583 0.006375806605617161]\n",
      " [(0, 1, 4, 5) 4.557355409578501 0.03277764946118493]\n",
      " [(0, 1, 4, 6) 15.631012242334467 7.69816541785587e-05]\n",
      " [(0, 1, 4, 7) 4.712792637565612 0.029938979144170846]\n",
      " [(0, 1, 4, 8) 5.259291598093579 0.021829902494388428]\n",
      " [(0, 1, 5, 6) 11.097090055952172 0.0008646328305015386]\n",
      " [(0, 1, 5, 7) 5.3415171539789945 0.02082334267725884]\n",
      " [(0, 1, 5, 8) 5.366727537522462 0.020524448999856122]\n",
      " [(0, 1, 6, 7) 12.66956230358232 0.0003716565264327385]\n",
      " [(0, 1, 6, 8) 13.006909987629882 0.0003103436322766449]\n",
      " [(0, 1, 6, 9) 8.999372127197173 0.002700723770746502]\n",
      " [(0, 1, 6, 10) 9.491299294662099 0.0020644856905616305]\n",
      " [(0, 1, 7, 8) 7.122121909068907 0.007613853971589716]\n",
      " [(0, 2, 3, 9) 3.4496594951349007 0.06326480186695908]\n",
      " [(0, 2, 3, 10) 3.1480024982604284 0.07601996961450626]\n",
      " [(0, 2, 7, 8) 6.782278013432707 0.009206731658832883]\n",
      " [(0, 2, 7, 9) 4.89169842117365 0.026986126140522634]\n",
      " [(0, 2, 7, 10) 8.907735927912132 0.0028396511186367015]\n",
      " [(0, 2, 8, 9) 5.840585456338916 0.015660614900285332]\n",
      " [(0, 2, 8, 10) 9.640980977407178 0.001902835850326422]\n",
      " [(0, 2, 9, 10) 8.41034030142522 0.003730928165439175]\n",
      " [(0, 4, 5, 6) 7.985563686477106 0.004715181210117219]\n",
      " [(0, 4, 6, 7) 8.176681192928148 0.004243232329590499]\n",
      " [(0, 4, 6, 8) 9.50643673591099 0.002047523774199105]\n",
      " [(0, 4, 6, 10) 5.78997936406442 0.016117778381122546]\n",
      " [(0, 5, 6, 7) 6.451657156394795 0.011084890281624071]\n",
      " [(0, 5, 6, 8) 6.808789631581714 0.009071023045074794]\n",
      " [(0, 5, 6, 9) 3.8027738682716947 0.051167749972554605]\n",
      " [(0, 5, 6, 10) 3.0314230622470557 0.08166636078114076]\n",
      " [(0, 6, 7, 8) 8.367495613360603 0.0038199167556773263]\n",
      " [(0, 6, 7, 10) 3.243034312378954 0.0717276816674813]\n",
      " [(0, 6, 8, 10) 3.435019689227701 0.06382782438180201]\n",
      " [(0, 7, 8, 9) 6.9960929136877965 0.008168781772057904]\n",
      " [(0, 7, 8, 10) 7.2906603884848655 0.006931399245529803]\n",
      " [(0, 7, 9, 10) 12.879207723971772 0.00033225286515422897]\n",
      " [(0, 8, 9, 10) 14.713808959708711 0.00012512653522007945]\n",
      " [(1, 2, 4, 6) 4.716389368463501 0.029876413537113546]\n",
      " [(1, 2, 5, 6) 7.732939612494308 0.005422240029646227]\n",
      " [(1, 2, 6, 7) 8.102533162490246 0.004420343857874204]\n",
      " [(1, 2, 6, 8) 7.770595144303007 0.005310355001183618]\n",
      " [(1, 2, 6, 9) 3.526358839834474 0.060400295586695754]\n",
      " [(1, 2, 6, 10) 4.446320387955932 0.034976533107811986]\n",
      " [(1, 3, 4, 6) 6.129869157964567 0.013291670742041852]\n",
      " [(1, 3, 5, 6) 4.723227378518508 0.029757840872066055]\n",
      " [(1, 3, 6, 7) 9.755359442371912 0.0017880067930011122]\n",
      " [(1, 3, 6, 8) 10.062230404166943 0.0015133989916104541]\n",
      " [(1, 3, 6, 9) 5.546364741374723 0.01851911052171475]\n",
      " [(1, 3, 6, 10) 6.24614204322415 0.012446410308298543]\n",
      " [(1, 3, 7, 8) 3.6643845632640835 0.05558716899230986]\n",
      " [(1, 4, 5, 6) 10.000772780406214 0.001564745505905921]\n",
      " [(1, 4, 5, 7) 5.475531532391345 0.019284495290275695]\n",
      " [(1, 4, 5, 8) 5.5189839729877646 0.01881118130883048]\n",
      " [(1, 4, 5, 10) 3.718899656684968 0.05379981399052391]\n",
      " [(1, 4, 6, 7) 18.32584909909258 1.8616397784496137e-05]\n",
      " [(1, 4, 6, 8) 19.944193235105693 7.973575022783678e-06]\n",
      " [(1, 4, 6, 9) 5.519015290163557 0.01881084454384468]\n",
      " [(1, 4, 6, 10) 15.241856401279982 9.458370359542167e-05]\n",
      " [(1, 4, 7, 8) 7.048197089169168 0.007934507384456002]\n",
      " [(1, 4, 7, 10) 4.173415156466577 0.04106295031449328]\n",
      " [(1, 4, 8, 10) 4.446321920599682 0.03497650171396769]\n",
      " [(1, 5, 6, 7) 12.7105836721555 0.00036359192760025417]\n",
      " [(1, 5, 6, 8) 11.732225495700714 0.0006142709235472918]\n",
      " [(1, 5, 6, 9) 8.84443568173644 0.0029398378198992664]\n",
      " [(1, 5, 6, 10) 9.251933331478154 0.002352469397196547]\n",
      " [(1, 5, 7, 8) 4.917046735248306 0.026592937557875444]\n",
      " [(1, 6, 7, 8) 15.799057525002766 7.04375861889824e-05]\n",
      " [(1, 6, 7, 9) 9.877140061000066 0.0016734484457649687]\n",
      " [(1, 6, 7, 10) 11.417069723711386 0.000727723812574367]\n",
      " [(1, 6, 8, 9) 10.653034544797471 0.0010989045871869082]\n",
      " [(1, 6, 8, 10) 11.495050611620984 0.0006978175816699379]\n",
      " [(1, 6, 9, 10) 6.323188422318288 0.01191691642332269]\n",
      " [(1, 7, 8, 10) 3.2061987137203314 0.07335973331793888]\n",
      " [(2, 3, 8, 9) 3.244232638033699 0.07167524646026353]\n",
      " [(2, 3, 9, 10) 5.135851489021283 0.023436588143788246]\n",
      " [(2, 7, 8, 9) 4.327186426295235 0.037508197471821596]\n",
      " [(2, 7, 8, 10) 11.676057697165895 0.000633095388992232]\n",
      " [(2, 7, 9, 10) 6.784343609870873 0.009196083917740945]\n",
      " [(2, 8, 9, 10) 8.186856582386145 0.00421949731153579]\n",
      " [(3, 4, 6, 7) 3.244588391375527 0.07165968765076314]\n",
      " [(3, 4, 6, 8) 3.283337491924544 0.06998647398682047]\n",
      " [(3, 7, 9, 10) 3.349337873051608 0.06723205169278892]\n",
      " [(3, 8, 9, 10) 3.3520785094980265 0.06712021193562548]\n",
      " [(4, 5, 6, 7) 6.974228148377339 0.008269188142518522]\n",
      " [(4, 5, 6, 8) 6.812782371965468 0.009050763054928645]\n",
      " [(4, 5, 6, 10) 5.259266314460709 0.021830219632563843]\n",
      " [(4, 6, 7, 8) 11.614682417701884 0.000654332100552025]\n",
      " [(4, 6, 7, 10) 7.16242260611971 0.007444658184420357]\n",
      " [(4, 6, 8, 9) 3.2748564874531985 0.07034907914456828]\n",
      " [(4, 6, 8, 10) 8.204263386883644 0.004179207213093086]\n",
      " [(5, 6, 7, 8) 5.552189748000047 0.01845758174810371]\n",
      " [(5, 6, 7, 9) 4.037849287163535 0.04449048718422483]\n",
      " [(5, 6, 8, 9) 4.1362720608825905 0.04197350980456662]\n",
      " [(6, 7, 8, 9) 3.1289836830848037 0.07691167419493095]\n",
      " [(6, 7, 8, 10) 3.540275318136144 0.05989551849885363]\n",
      " [(7, 8, 9, 10) 14.21966566354494 0.00016266167764751263]]\n",
      "number of features:  194\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "# 1599 red win\n",
    "wine_path = \"./Data/winequality-red.csv\"\n",
    "data = np.genfromtxt(wine_path, delimiter=\";\", skip_header=1)\n",
    "# with open(wine_path) as f:\n",
    "#     f.readline()\n",
    "#     data = np.loadtxt(f, delimiter=\";\")\n",
    "# f = open(wine_path)\n",
    "# f.readline()  # skip the header\n",
    "# data = np.loadtxt(f, delimiter=\";\")\n",
    "# f.close()\n",
    "\n",
    "X_wine = data[:, 0:11]  # select columns 1 through 11\n",
    "y_wine = data[:, 11]\n",
    "\n",
    "# convert y values to 0, 1\n",
    "y_wine = np.where(y_wine < 5.5, 0, 1)\n",
    "# print(y_wine[:100])\n",
    "# wine = list(zip(X_wine, y_wine))\n",
    "# print(\"Total number of wine\", len(wine))\n",
    "# print(X_wine[0])\n",
    "# print(y_wine[0])\n",
    "# print(wine[0])\n",
    "\n",
    "# standardize\n",
    "#X_wine = standardize(X_wine)\n",
    "\n",
    "# adding interaction terms\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "comblist = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "comb = combinations(comblist, 2)\n",
    "\n",
    "i = 11\n",
    "for (a, b) in comb:\n",
    "    # print((a, b), i)\n",
    "    list.append((a, b))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "comb3 = combinations(comblist, 3)\n",
    "for (a, b, c) in comb3:\n",
    "    #print((a, b, c), i)\n",
    "    list.append((a, b, c))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "comb4 = combinations(comblist, 4)\n",
    "for (a, b, c, d) in comb4:\n",
    "    #print((a, b, c, d), i)\n",
    "    list.append((a, b, c, d))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "#\n",
    "#comb5 = combinations(comblist, 5)\n",
    "#for (a, b, c, d, e) in comb5:\n",
    "#    #print((a, b, c, d, e), i)\n",
    "#    list.append((a, b, c, d, e))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]  * X_wine[:, e]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "    \n",
    "\n",
    "# normalize\n",
    "X_wine = normalize(X_wine)\n",
    "\n",
    "# chi squared table todo\n",
    "chi, pval = chi2(X_wine, y_wine)\n",
    "list = np.asarray(list)\n",
    "chilist = np.column_stack((list, chi, pval))\n",
    "\n",
    "# threshold of chisquare interaction\n",
    "index_of_chi = np.where(chi < 3)\n",
    "#keep no interaction terms\n",
    "index_of_chi = np.delete(index_of_chi, np.where(index_of_chi[0] < 11), axis=1)\n",
    "# remove comment to delete insignificant interaction rows\n",
    "X_wine = np.delete(X_wine, index_of_chi , axis=1)\n",
    "chilist = np.delete(chilist, index_of_chi, axis=0)\n",
    "print(chilist)\n",
    "print(\"number of features: \",len(chilist))\n",
    "\n",
    "# variance threshold on features\n",
    "# print(X_wine.var(axis=0))\n",
    "# for lda removing features decreased accuracy\n",
    "# index_of_var = np.where(X_wine.var(axis=0) < 0.00000001)\n",
    "# X_wine = np.delete(X_wine, index_of_var[0] , axis=1)\n",
    "\n",
    "\n",
    "# recombininng and shuffling\n",
    "wine = np.column_stack((X_wine, y_wine))\n",
    "#np.random.shuffle(wine)\n",
    "\n",
    "# split into train test set\n",
    "n = X_wine.shape[0]\n",
    "f_size = X_wine.shape[1]\n",
    "X_wine = wine[(n // 10) :, 0:f_size]\n",
    "X_wine_test = wine[: (n // 10), 0:f_size]\n",
    "y_wine = wine[(n // 10) :, f_size].reshape(-1,1)\n",
    "y_wine_test = wine[: (n // 10), f_size].reshape(-1,1)\n",
    "\n",
    "# standardize\n",
    "\n",
    "#X_wine = normalize(X_wine)\n",
    "#X_wine_test= normalize(X_wine_test)\n",
    "\n",
    "\n",
    "# qualities = [float(item[-1]) for item in wines[1:]]\n",
    "# sum(qualities) / len(qualities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  561\n",
      "0.7388194444444445\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  331\n",
      "0.7499305555555555\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  252\n",
      "0.750486111111111\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  194\n",
      "0.750138888888889\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 way interaction (notice increase in accuracy to 75.1%)\n",
    "## best accuracy around 300 features (chi < 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1.042185209008817 0.3073137941896489]\n",
      " [1 8.891043306316842 0.0028657293536039487]\n",
      " [2 5.666702872536845 0.01728992371507948]\n",
      " ...\n",
      " [(5, 6, 7, 8, 10) 2.9813658363089584 0.08422816680819549]\n",
      " [(5, 6, 7, 9, 10) 2.0649364972954003 0.15072120702755837]\n",
      " [(5, 6, 8, 9, 10) 2.0299253126784547 0.15422818633148064]]\n",
      "413\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "# 1599 red win\n",
    "wine_path = \"./Data/winequality-red.csv\"\n",
    "data = np.genfromtxt(wine_path, delimiter=\";\", skip_header=1)\n",
    "# with open(wine_path) as f:\n",
    "#     f.readline()\n",
    "#     data = np.loadtxt(f, delimiter=\";\")\n",
    "# f = open(wine_path)\n",
    "# f.readline()  # skip the header\n",
    "# data = np.loadtxt(f, delimiter=\";\")\n",
    "# f.close()\n",
    "\n",
    "X_wine = data[:, 0:11]  # select columns 1 through 11\n",
    "y_wine = data[:, 11]\n",
    "\n",
    "# convert y values to 0, 1\n",
    "y_wine = np.where(y_wine < 5.5, 0, 1)\n",
    "# print(y_wine[:100])\n",
    "# wine = list(zip(X_wine, y_wine))\n",
    "# print(\"Total number of wine\", len(wine))\n",
    "# print(X_wine[0])\n",
    "# print(y_wine[0])\n",
    "# print(wine[0])\n",
    "\n",
    "# standardize\n",
    "#X_wine = standardize(X_wine)\n",
    "\n",
    "# adding interaction terms\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "comblist = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "comb = combinations(comblist, 2)\n",
    "\n",
    "i = 11\n",
    "for (a, b) in comb:\n",
    "    # print((a, b), i)\n",
    "    list.append((a, b))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "comb3 = combinations(comblist, 3)\n",
    "for (a, b, c) in comb3:\n",
    "    #print((a, b, c), i)\n",
    "    list.append((a, b, c))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "comb4 = combinations(comblist, 4)\n",
    "for (a, b, c, d) in comb4:\n",
    "    #print((a, b, c, d), i)\n",
    "    list.append((a, b, c, d))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "comb5 = combinations(comblist, 5)\n",
    "for (a, b, c, d, e) in comb5:\n",
    "    #print((a, b, c, d, e), i)\n",
    "    list.append((a, b, c, d, e))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]  * X_wine[:, e]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "    \n",
    "\n",
    "# normalize\n",
    "X_wine = normalize(X_wine)\n",
    "\n",
    "# chi squared table todo\n",
    "chi, pval = chi2(X_wine, y_wine)\n",
    "list = np.asarray(list)\n",
    "chilist = np.column_stack((list, chi, pval))\n",
    "\n",
    "# threshold of chisquare interaction\n",
    "index_of_chi = np.where(chi < 2)\n",
    "#keep no interaction terms\n",
    "index_of_chi = np.delete(index_of_chi, np.where(index_of_chi[0] < 11), axis=1)\n",
    "# remove comment to delete insignificant interaction rows\n",
    "X_wine = np.delete(X_wine, index_of_chi , axis=1)\n",
    "chilist = np.delete(chilist, index_of_chi, axis=0)\n",
    "print(chilist)\n",
    "print(len(chilist))\n",
    "\n",
    "# variance threshold on features\n",
    "# print(X_wine.var(axis=0))\n",
    "# for lda removing features decreased accuracy\n",
    "# index_of_var = np.where(X_wine.var(axis=0) < 0.00000001)\n",
    "# X_wine = np.delete(X_wine, index_of_var[0] , axis=1)\n",
    "\n",
    "\n",
    "# recombininng and shuffling\n",
    "wine = np.column_stack((X_wine, y_wine))\n",
    "#np.random.shuffle(wine)\n",
    "\n",
    "# split into train test set\n",
    "n = X_wine.shape[0]\n",
    "f_size = X_wine.shape[1]\n",
    "X_wine = wine[(n // 10) :, 0:f_size]\n",
    "X_wine_test = wine[: (n // 10), 0:f_size]\n",
    "y_wine = wine[(n // 10) :, f_size].reshape(-1,1)\n",
    "y_wine_test = wine[: (n // 10), f_size].reshape(-1,1)\n",
    "\n",
    "# standardize\n",
    "\n",
    "#X_wine = normalize(X_wine)\n",
    "#X_wine_test= normalize(X_wine_test)\n",
    "\n",
    "\n",
    "# qualities = [float(item[-1]) for item in wines[1:]]\n",
    "# sum(qualities) / len(qualities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  874\n",
      "0.7279861111111111\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  580\n",
      "0.7504166666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  413\n",
      "0.7509722222222222\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  315\n",
      "0.7486111111111111\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  253\n",
      "0.7473611111111111\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  213\n",
      "0.7470833333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "#print(score)\n",
    "# print(scorec)\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))\n",
    "# test runtime\n",
    "# timeit.timeit(ld.fit(X_train, y_train), number=10)??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing features 0,3,4,5,7,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 8.89104331e+00 2.86572935e-03]\n",
      " [1.00000000e+00 5.66670287e+00 1.72899237e-02]\n",
      " [2.00000000e+00 8.12431060e+00 4.36755887e-03]\n",
      " [3.00000000e+00 3.98440125e+00 4.59234196e-02]\n",
      " [4.00000000e+00 2.60853253e+01 3.26657278e-07]]\n",
      "number of features:  5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "# 1599 red win\n",
    "wine_path = \"./Data/winequality-red.csv\"\n",
    "data = np.genfromtxt(wine_path, delimiter=\";\", skip_header=1)\n",
    "\n",
    "X_wine = data[:, [1,2,6,9,10]]  # select columns 1 through 11\n",
    "y_wine = data[:, 11]\n",
    "\n",
    "# convert y values to 0, 1\n",
    "y_wine = np.where(y_wine < 5.5, 0, 1)\n",
    "\n",
    "# standardize\n",
    "#X_wine = standardize(X_wine)\n",
    "\n",
    "# adding interaction terms\n",
    "\n",
    "from itertools import combinations\n",
    "list = [0,1,2,3,4]\n",
    "comblist = [0,1,2,3,4]\n",
    "comb = combinations(comblist, 2)\n",
    "\n",
    "#i = 7\n",
    "#for (a, b) in comb:\n",
    "#    # print((a, b), i)\n",
    "#    list.append((a, b))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "#comb3 = combinations(comblist, 3)\n",
    "#for (a, b, c) in comb3:\n",
    "#    #print((a, b, c), i)\n",
    "#    list.append((a, b, c))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "#comb4 = combinations(comblist, 4)\n",
    "#for (a, b, c, d) in comb4:\n",
    "#    #print((a, b, c, d), i)\n",
    "#    list.append((a, b, c, d))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "#comb5 = combinations(comblist, 5)\n",
    "#for (a, b, c, d, e) in comb5:\n",
    "#    #print((a, b, c, d, e), i)\n",
    "#    list.append((a, b, c, d, e))\n",
    "#    i += 1\n",
    "#    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]  * X_wine[:, e]\n",
    "#    X_wine = np.column_stack((X_wine, inter))\n",
    "    \n",
    "\n",
    "# normalize\n",
    "X_wine = normalize(X_wine)\n",
    "\n",
    "# chi squared table todo\n",
    "chi, pval = chi2(X_wine, y_wine)\n",
    "list = np.asarray(list)\n",
    "chilist = np.column_stack((list, chi, pval))\n",
    "\n",
    "# threshold of chisquare interaction\n",
    "index_of_chi = np.where(chi < 1)\n",
    "#keep no interaction terms\n",
    "index_of_chi = np.delete(index_of_chi, np.where(index_of_chi[0] < 5), axis=1)\n",
    "# remove comment to delete insignificant interaction rows\n",
    "X_wine = np.delete(X_wine, index_of_chi , axis=1)\n",
    "chilist = np.delete(chilist, index_of_chi, axis=0)\n",
    "print(chilist)\n",
    "print(\"number of features: \",len(chilist))\n",
    "print(X_wine.shape[1])\n",
    "\n",
    "\n",
    "# recombininng and shuffling\n",
    "wine = np.column_stack((X_wine, y_wine))\n",
    "#np.random.shuffle(wine)\n",
    "\n",
    "# split into train test set\n",
    "n = X_wine.shape[0]\n",
    "f_size = X_wine.shape[1]\n",
    "X_wine = wine[:, 0:f_size]\n",
    "#X_wine_test = wine[:, 0:f_size]\n",
    "y_wine = wine[:, f_size].reshape(-1,1)\n",
    "#y_wine_test = wine[:, f_size].reshape(-1,1)\n",
    "\n",
    "# standardize\n",
    "\n",
    "#X_wine = normalize(X_wine)\n",
    "#X_wine_test= normalize(X_wine_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features with chi1:  5\n",
      "0.7410846394984325\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features with chi1: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features with chi1:  19\n",
      "0.740651645768025\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features with chi1: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  15\n",
      "0.7410325235109718\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  25\n",
      "0.740587382445141\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  31\n",
      "0.7383336598746082\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 way interaction similar to 5-way best 75.2% at chi = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1.042185209008817 0.3073137941896489]\n",
      " [1 8.891043306316842 0.0028657293536039487]\n",
      " [2 5.666702872536845 0.01728992371507948]\n",
      " [3 0.0006196962034192906 0.9801397538925941]\n",
      " [4 0.93881024263718 0.33258400473573235]\n",
      " [5 0.6314063119772713 0.4268399382497391]\n",
      " [6 8.124310600120378 0.0043675588718536905]\n",
      " [7 1.5846430385807617 0.20809316689519386]\n",
      " [8 0.0005594550998740942 0.9811295530403128]\n",
      " [9 3.9844012485431595 0.045923419633608194]\n",
      " [10 26.08532534641502 3.2665727821800035e-07]\n",
      " [(0, 1) 7.326107125644496 0.0067960088524447555]\n",
      " [(0, 2) 6.326521347333114 0.01189454026078067]\n",
      " [(0, 6) 7.657429213747134 0.005653901097783591]\n",
      " [(0, 9) 5.930387759132255 0.01488196682923952]\n",
      " [(0, 10) 6.43939725454695 0.011161651229169232]\n",
      " [(1, 4) 6.563016603867104 0.010411971648553951]\n",
      " [(1, 5) 5.41122870170849 0.02000762852905962]\n",
      " [(1, 6) 15.512383994186958 8.196649778217025e-05]\n",
      " [(1, 7) 8.949767167109703 0.0027750495281751927]\n",
      " [(1, 8) 8.388006917260578 0.003777048435039452]\n",
      " [(2, 7) 5.622386899725173 0.017732482950550174]\n",
      " [(2, 8) 6.6680193286477945 0.009815821564078193]\n",
      " [(2, 10) 11.576937326250402 0.0006677483024442473]\n",
      " [(4, 6) 10.043763191404892 0.001528646013510178]\n",
      " [(5, 6) 5.789898831998592 0.016118516748681994]\n",
      " [(6, 7) 8.19399978601782 0.004202915911067641]\n",
      " [(6, 8) 8.563313052113898 0.0034300465235166167]\n",
      " [(7, 10) 26.023091781898767 3.37358053359128e-07]\n",
      " [(8, 9) 4.861879799046382 0.027456405078770025]\n",
      " [(8, 10) 12.6636372549147 0.0003728361812785301]\n",
      " [(9, 10) 12.05482508856085 0.0005165849504810979]\n",
      " [(0, 1, 4) 4.717076750941336 0.02986447194296387]\n",
      " [(0, 1, 5) 5.32018448945499 0.021079779510703604]\n",
      " [(0, 1, 6) 12.65620877588664 0.0003743205007183088]\n",
      " [(0, 1, 7) 7.345817614863097 0.006721895361822601]\n",
      " [(0, 1, 8) 7.0784595831532275 0.0078016048112224115]\n",
      " [(0, 2, 7) 6.266026751438178 0.012307482552397738]\n",
      " [(0, 2, 8) 6.8484585545747025 0.00887177840232707]\n",
      " [(0, 2, 9) 4.915511485679668 0.02661658119885388]\n",
      " [(0, 2, 10) 8.939177284289215 0.0027911840847677856]\n",
      " [(0, 4, 6) 8.1801971630286 0.004235015683255917]\n",
      " [(0, 5, 6) 6.397371192891343 0.011428947278196603]\n",
      " [(0, 6, 7) 7.717399173922749 0.005469112899213384]\n",
      " [(0, 6, 8) 8.301540196533228 0.003961147086714267]\n",
      " [(0, 7, 9) 5.858555243955312 0.015501513115351246]\n",
      " [(0, 7, 10) 6.364071087261573 0.0116454043416054]\n",
      " [(0, 8, 9) 7.085453065283974 0.007771216964022078]\n",
      " [(0, 8, 10) 7.380715513434366 0.006592693618878551]\n",
      " [(0, 9, 10) 12.946068228611209 0.00032059410849762213]\n",
      " [(1, 2, 6) 8.087889944234474 0.0044562014316277335]\n",
      " [(1, 3, 6) 9.794476869842011 0.0017503679872816142]\n",
      " [(1, 4, 5) 5.472659685897707 0.019316207784072448]\n",
      " [(1, 4, 6) 18.345677297039074 1.842365176622988e-05]\n",
      " [(1, 4, 7) 6.579988334790726 0.010313156841761352]\n",
      " [(1, 4, 8) 7.029226754037694 0.008018997088568786]\n",
      " [(1, 4, 10) 4.171722890972551 0.0411039815855198]\n",
      " [(1, 5, 6) 12.702507187052179 0.0003651656487460378]\n",
      " [(1, 5, 7) 5.428018812910889 0.019816148008396164]\n",
      " [(1, 5, 8) 4.900429205741495 0.026850021296118784]\n",
      " [(1, 6, 7) 15.539695190638414 8.079082182198535e-05]\n",
      " [(1, 6, 8) 15.769472029577393 7.154771070977296e-05]\n",
      " [(1, 6, 9) 9.846435341308908 0.0017016116879401116]\n",
      " [(1, 6, 10) 11.385322158631128 0.0007402671326848113]\n",
      " [(1, 7, 8) 8.44492981738627 0.003660625641881543]\n",
      " [(2, 7, 8) 6.6150973089293785 0.01011178139030116]\n",
      " [(2, 7, 10) 11.473835331524166 0.0007058286640908061]\n",
      " [(2, 8, 9) 4.354230383541616 0.03691713619663238]\n",
      " [(2, 8, 10) 11.781826298852685 0.0005981177608834917]\n",
      " [(2, 9, 10) 6.823062274882353 0.008998813637441781]\n",
      " [(4, 5, 6) 6.960268214476988 0.008333953871305876]\n",
      " [(4, 6, 7) 10.0427810690693 0.0015294612229701525]\n",
      " [(4, 6, 8) 11.616528218091851 0.000653683051907268]\n",
      " [(4, 6, 10) 7.155602285547733 0.007473019656979733]\n",
      " [(5, 6, 7) 5.818884301445614 0.015854998990061913]\n",
      " [(5, 6, 8) 5.523696484023612 0.01876057594622148]\n",
      " [(5, 6, 9) 4.010004280720782 0.04523103593794565]\n",
      " [(6, 7, 8) 8.640888866623008 0.003287010209313742]\n",
      " [(7, 8, 9) 4.809010544375845 0.028311296466237035]\n",
      " [(7, 8, 10) 12.749873739690702 0.00035603319628734994]\n",
      " [(7, 9, 10) 11.972732535565019 0.0005398471772698004]\n",
      " [(8, 9, 10) 14.318768506303071 0.00015431859977841722]\n",
      " [(0, 1, 2, 6) 5.8817415460464595 0.015298682288942421]\n",
      " [(0, 1, 3, 6) 7.440898089865583 0.006375806605617161]\n",
      " [(0, 1, 4, 5) 4.557355409578501 0.03277764946118493]\n",
      " [(0, 1, 4, 6) 15.631012242334467 7.69816541785587e-05]\n",
      " [(0, 1, 4, 7) 4.712792637565612 0.029938979144170846]\n",
      " [(0, 1, 4, 8) 5.259291598093579 0.021829902494388428]\n",
      " [(0, 1, 5, 6) 11.097090055952172 0.0008646328305015386]\n",
      " [(0, 1, 5, 7) 5.3415171539789945 0.02082334267725884]\n",
      " [(0, 1, 5, 8) 5.366727537522462 0.020524448999856122]\n",
      " [(0, 1, 6, 7) 12.66956230358232 0.0003716565264327385]\n",
      " [(0, 1, 6, 8) 13.006909987629882 0.0003103436322766449]\n",
      " [(0, 1, 6, 9) 8.999372127197173 0.002700723770746502]\n",
      " [(0, 1, 6, 10) 9.491299294662099 0.0020644856905616305]\n",
      " [(0, 1, 7, 8) 7.122121909068907 0.007613853971589716]\n",
      " [(0, 2, 7, 8) 6.782278013432707 0.009206731658832883]\n",
      " [(0, 2, 7, 9) 4.89169842117365 0.026986126140522634]\n",
      " [(0, 2, 7, 10) 8.907735927912132 0.0028396511186367015]\n",
      " [(0, 2, 8, 9) 5.840585456338916 0.015660614900285332]\n",
      " [(0, 2, 8, 10) 9.640980977407178 0.001902835850326422]\n",
      " [(0, 2, 9, 10) 8.41034030142522 0.003730928165439175]\n",
      " [(0, 4, 5, 6) 7.985563686477106 0.004715181210117219]\n",
      " [(0, 4, 6, 7) 8.176681192928148 0.004243232329590499]\n",
      " [(0, 4, 6, 8) 9.50643673591099 0.002047523774199105]\n",
      " [(0, 4, 6, 10) 5.78997936406442 0.016117778381122546]\n",
      " [(0, 5, 6, 7) 6.451657156394795 0.011084890281624071]\n",
      " [(0, 5, 6, 8) 6.808789631581714 0.009071023045074794]\n",
      " [(0, 6, 7, 8) 8.367495613360603 0.0038199167556773263]\n",
      " [(0, 7, 8, 9) 6.9960929136877965 0.008168781772057904]\n",
      " [(0, 7, 8, 10) 7.2906603884848655 0.006931399245529803]\n",
      " [(0, 7, 9, 10) 12.879207723971772 0.00033225286515422897]\n",
      " [(0, 8, 9, 10) 14.713808959708711 0.00012512653522007945]\n",
      " [(1, 2, 4, 6) 4.716389368463501 0.029876413537113546]\n",
      " [(1, 2, 5, 6) 7.732939612494308 0.005422240029646227]\n",
      " [(1, 2, 6, 7) 8.102533162490246 0.004420343857874204]\n",
      " [(1, 2, 6, 8) 7.770595144303007 0.005310355001183618]\n",
      " [(1, 2, 6, 10) 4.446320387955932 0.034976533107811986]\n",
      " [(1, 3, 4, 6) 6.129869157964567 0.013291670742041852]\n",
      " [(1, 3, 5, 6) 4.723227378518508 0.029757840872066055]\n",
      " [(1, 3, 6, 7) 9.755359442371912 0.0017880067930011122]\n",
      " [(1, 3, 6, 8) 10.062230404166943 0.0015133989916104541]\n",
      " [(1, 3, 6, 9) 5.546364741374723 0.01851911052171475]\n",
      " [(1, 3, 6, 10) 6.24614204322415 0.012446410308298543]\n",
      " [(1, 4, 5, 6) 10.000772780406214 0.001564745505905921]\n",
      " [(1, 4, 5, 7) 5.475531532391345 0.019284495290275695]\n",
      " [(1, 4, 5, 8) 5.5189839729877646 0.01881118130883048]\n",
      " [(1, 4, 6, 7) 18.32584909909258 1.8616397784496137e-05]\n",
      " [(1, 4, 6, 8) 19.944193235105693 7.973575022783678e-06]\n",
      " [(1, 4, 6, 9) 5.519015290163557 0.01881084454384468]\n",
      " [(1, 4, 6, 10) 15.241856401279982 9.458370359542167e-05]\n",
      " [(1, 4, 7, 8) 7.048197089169168 0.007934507384456002]\n",
      " [(1, 4, 7, 10) 4.173415156466577 0.04106295031449328]\n",
      " [(1, 4, 8, 10) 4.446321920599682 0.03497650171396769]\n",
      " [(1, 5, 6, 7) 12.7105836721555 0.00036359192760025417]\n",
      " [(1, 5, 6, 8) 11.732225495700714 0.0006142709235472918]\n",
      " [(1, 5, 6, 9) 8.84443568173644 0.0029398378198992664]\n",
      " [(1, 5, 6, 10) 9.251933331478154 0.002352469397196547]\n",
      " [(1, 5, 7, 8) 4.917046735248306 0.026592937557875444]\n",
      " [(1, 6, 7, 8) 15.799057525002766 7.04375861889824e-05]\n",
      " [(1, 6, 7, 9) 9.877140061000066 0.0016734484457649687]\n",
      " [(1, 6, 7, 10) 11.417069723711386 0.000727723812574367]\n",
      " [(1, 6, 8, 9) 10.653034544797471 0.0010989045871869082]\n",
      " [(1, 6, 8, 10) 11.495050611620984 0.0006978175816699379]\n",
      " [(1, 6, 9, 10) 6.323188422318288 0.01191691642332269]\n",
      " [(2, 3, 9, 10) 5.135851489021283 0.023436588143788246]\n",
      " [(2, 7, 8, 9) 4.327186426295235 0.037508197471821596]\n",
      " [(2, 7, 8, 10) 11.676057697165895 0.000633095388992232]\n",
      " [(2, 7, 9, 10) 6.784343609870873 0.009196083917740945]\n",
      " [(2, 8, 9, 10) 8.186856582386145 0.00421949731153579]\n",
      " [(4, 5, 6, 7) 6.974228148377339 0.008269188142518522]\n",
      " [(4, 5, 6, 8) 6.812782371965468 0.009050763054928645]\n",
      " [(4, 5, 6, 10) 5.259266314460709 0.021830219632563843]\n",
      " [(4, 6, 7, 8) 11.614682417701884 0.000654332100552025]\n",
      " [(4, 6, 7, 10) 7.16242260611971 0.007444658184420357]\n",
      " [(4, 6, 8, 10) 8.204263386883644 0.004179207213093086]\n",
      " [(5, 6, 7, 8) 5.552189748000047 0.01845758174810371]\n",
      " [(5, 6, 7, 9) 4.037849287163535 0.04449048718422483]\n",
      " [(5, 6, 8, 9) 4.1362720608825905 0.04197350980456662]\n",
      " [(7, 8, 9, 10) 14.21966566354494 0.00016266167764751263]\n",
      " [(0, 1, 2, 5, 6) 7.088602060496328 0.007757573596330189]\n",
      " [(0, 1, 2, 6, 7) 5.88992213260999 0.015227773695192895]\n",
      " [(0, 1, 2, 6, 8) 6.104784494338601 0.013481638925030268]\n",
      " [(0, 1, 3, 4, 6) 4.559266004547943 0.032741102424751894]\n",
      " [(0, 1, 3, 5, 6) 4.148368074116009 0.041674670156599626]\n",
      " [(0, 1, 3, 6, 7) 7.406861072737135 0.006497559156657025]\n",
      " [(0, 1, 3, 6, 8) 7.697122120751918 0.0055308941838386845]\n",
      " [(0, 1, 3, 6, 9) 4.613927778353709 0.03171331575967418]\n",
      " [(0, 1, 3, 6, 10) 5.942188517910099 0.014782651835979934]\n",
      " [(0, 1, 4, 5, 6) 11.699996565242724 0.0006250021321878717]\n",
      " [(0, 1, 4, 5, 7) 4.558430522440157 0.03275707880297784]\n",
      " [(0, 1, 4, 5, 8) 5.224789220785764 0.02226714494506827]\n",
      " [(0, 1, 4, 6, 7) 15.610946395107408 7.780288067947989e-05]\n",
      " [(0, 1, 4, 6, 8) 18.105454064728292 2.090019144698306e-05]\n",
      " [(0, 1, 4, 6, 9) 4.716726508957254 0.029870555921689624]\n",
      " [(0, 1, 4, 6, 10) 12.95464801094651 0.0003191281850138006]\n",
      " [(0, 1, 4, 7, 8) 5.272940420581677 0.021659396861015603]\n",
      " [(0, 1, 5, 6, 7) 11.115422842726353 0.0008561280405827012]\n",
      " [(0, 1, 5, 6, 8) 11.34395061820124 0.000756941287692357]\n",
      " [(0, 1, 5, 6, 9) 8.186412303705879 0.004220530805058173]\n",
      " [(0, 1, 5, 6, 10) 8.603494595522989 0.0033551858461192815]\n",
      " [(0, 1, 5, 7, 8) 5.389540386415392 0.02025780592763637]\n",
      " [(0, 1, 6, 7, 8) 13.021973785168509 0.00030785716461129067]\n",
      " [(0, 1, 6, 7, 9) 9.019313851032795 0.002671416559485372]\n",
      " [(0, 1, 6, 7, 10) 9.509073457015951 0.0020445837390658376]\n",
      " [(0, 1, 6, 8, 9) 9.898126005089335 0.0016544716610254423]\n",
      " [(0, 1, 6, 8, 10) 9.7519093389888 0.0017913655934927233]\n",
      " [(0, 1, 6, 9, 10) 5.852281771072572 0.015556867420212908]\n",
      " [(0, 2, 7, 8, 9) 5.811981059807151 0.015917353414139232]\n",
      " [(0, 2, 7, 8, 10) 9.606461992912617 0.0019389384662274896]\n",
      " [(0, 2, 7, 9, 10) 8.37092902470191 0.00381270663863481]\n",
      " [(0, 2, 8, 9, 10) 10.003835502562172 0.001562145376620703]\n",
      " [(0, 4, 5, 6, 7) 7.985036606440895 0.004716554161368398]\n",
      " [(0, 4, 5, 6, 8) 7.905878047866963 0.004927441830086129]\n",
      " [(0, 4, 5, 6, 10) 6.128862811551454 0.013299238633349138]\n",
      " [(0, 4, 6, 7, 8) 9.503116430582924 0.002051232128178003]\n",
      " [(0, 4, 6, 7, 10) 5.7928411220745115 0.016091562642063104]\n",
      " [(0, 4, 6, 8, 10) 6.679694450909109 0.009751733922330409]\n",
      " [(0, 5, 6, 7, 8) 6.867090539555318 0.008779747666104154]\n",
      " [(0, 7, 8, 9, 10) 14.6359834923661 0.00013040080782588116]\n",
      " [(1, 2, 4, 5, 6) 4.606906626935631 0.031843425337433444]\n",
      " [(1, 2, 4, 6, 7) 4.711524395879904 0.029961072919550902]\n",
      " [(1, 2, 4, 6, 8) 5.4068199801362615 0.020058223655762767]\n",
      " [(1, 2, 5, 6, 7) 7.7581699304417455 0.0053470111760506225]\n",
      " [(1, 2, 5, 6, 8) 7.791033253666132 0.005250615939683176]\n",
      " [(1, 2, 5, 6, 9) 4.362653610589689 0.03673504044980991]\n",
      " [(1, 2, 5, 6, 10) 4.501288824955917 0.03386931686157305]\n",
      " [(1, 2, 6, 7, 8) 7.8198682414777965 0.005167496298383364]\n",
      " [(1, 2, 6, 7, 10) 4.481495565728224 0.034263725383642876]\n",
      " [(1, 2, 6, 8, 9) 4.101929087716082 0.042834313857628994]\n",
      " [(1, 2, 6, 8, 10) 4.231214769438243 0.03968702623754072]\n",
      " [(1, 3, 4, 6, 7) 6.098366375652163 0.013530690935086844]\n",
      " [(1, 3, 4, 6, 8) 6.228969505963457 0.01256768527112184]\n",
      " [(1, 3, 4, 6, 9) 4.339400342497665 0.037240037255994986]\n",
      " [(1, 3, 4, 6, 10) 5.368576301884306 0.020502705538937196]\n",
      " [(1, 3, 5, 6, 7) 4.726140508815629 0.029707475555041662]\n",
      " [(1, 3, 5, 6, 8) 4.463056396207013 0.03463547214329716]\n",
      " [(1, 3, 6, 7, 8) 10.022843077146973 0.0015461062648649644]\n",
      " [(1, 3, 6, 7, 9) 5.542752177663834 0.018557375952925188]\n",
      " [(1, 3, 6, 7, 10) 6.294791479768244 0.012109326161736537]\n",
      " [(1, 3, 6, 8, 9) 5.433510056272356 0.019753935894021767]\n",
      " [(1, 3, 6, 8, 10) 6.773247195495654 0.009253432212407995]\n",
      " [(1, 3, 6, 9, 10) 4.016570852973282 0.04505523359466014]\n",
      " [(1, 4, 5, 6, 7) 10.011813628855046 0.0015553927926641677]\n",
      " [(1, 4, 5, 6, 8) 9.897488046886671 0.0016550453167286164]\n",
      " [(1, 4, 5, 6, 9) 4.930681762219624 0.026383906517529192]\n",
      " [(1, 4, 5, 6, 10) 8.391316939259493 0.003770176534915013]\n",
      " [(1, 4, 5, 7, 8) 5.532364410711999 0.01866786222366859]\n",
      " [(1, 4, 6, 7, 8) 19.992003721024155 7.776669076513955e-06]\n",
      " [(1, 4, 6, 7, 9) 5.51509795363058 0.01885301746712464]\n",
      " [(1, 4, 6, 7, 10) 15.232953587808655 9.503061536400025e-05]\n",
      " [(1, 4, 6, 8, 9) 6.288712773327468 0.012150926709772364]\n",
      " [(1, 4, 6, 8, 10) 16.858271934328478 4.0277401602548655e-05]\n",
      " [(1, 4, 6, 9, 10) 4.448792597357581 0.03492593200222922]\n",
      " [(1, 4, 7, 8, 10) 4.464175624184892 0.03461278795110677]\n",
      " [(1, 5, 6, 7, 8) 11.740764145148557 0.0006114591329863301]\n",
      " [(1, 5, 6, 7, 9) 8.863484595573258 0.0029093166932117227]\n",
      " [(1, 5, 6, 7, 10) 9.267228321004893 0.002332904729250037]\n",
      " [(1, 5, 6, 8, 9) 8.845499372968447 0.0029381249781683058]\n",
      " [(1, 5, 6, 8, 10) 8.498020631824822 0.003555330398271328]\n",
      " [(1, 5, 6, 9, 10) 6.481734732773157 0.01089885635498042]\n",
      " [(1, 6, 7, 8, 9) 10.688368943996725 0.0010781121834585368]\n",
      " [(1, 6, 7, 8, 10) 11.52907849904573 0.0006851596817967877]\n",
      " [(1, 6, 7, 9, 10) 6.354772874608765 0.011706592914513467]\n",
      " [(1, 6, 8, 9, 10) 6.743256569881284 0.009410269092545439]\n",
      " [(2, 3, 7, 9, 10) 5.1068140053548685 0.023832045821449956]\n",
      " [(2, 3, 8, 9, 10) 5.823792457128295 0.015810818598389054]\n",
      " [(2, 7, 8, 9, 10) 8.140106149024225 0.004329674910464467]\n",
      " [(4, 5, 6, 7, 8) 6.827037148517597 0.008978808577099717]\n",
      " [(4, 5, 6, 7, 10) 5.275128448273906 0.02163219181870713]\n",
      " [(4, 5, 6, 8, 10) 5.114687261793792 0.02372414192180045]\n",
      " [(4, 6, 7, 8, 10) 8.213362857685267 0.004158301481092311]\n",
      " [(5, 6, 7, 8, 9) 4.157383029575504 0.041453404335722345]\n",
      " [(0, 1, 2, 4, 6, 8) 4.389132908599014 0.03616870317859775]\n",
      " [(0, 1, 2, 5, 6, 7) 7.101657435475671 0.007701270480360004]\n",
      " [(0, 1, 2, 5, 6, 8) 7.306584198583867 0.006870239954387703]\n",
      " [(0, 1, 2, 5, 6, 10) 4.517463551722525 0.03355053612437446]\n",
      " [(0, 1, 2, 6, 7, 8) 6.113644176219573 0.013414226961746246]\n",
      " [(0, 1, 3, 4, 6, 7) 4.533971123018 0.03322842593766604]\n",
      " [(0, 1, 3, 4, 6, 8) 4.640774233150455 0.031220914211494883]\n",
      " [(0, 1, 3, 5, 6, 7) 4.131009107449121 0.042104236654704905]\n",
      " [(0, 1, 3, 5, 6, 8) 4.285774321950393 0.03843257294617524]\n",
      " [(0, 1, 3, 6, 7, 8) 7.662401667750025 0.00563834002638286]\n",
      " [(0, 1, 3, 6, 7, 9) 4.595436276578316 0.032057183049989925]\n",
      " [(0, 1, 3, 6, 7, 10) 5.921274174777259 0.014959136623753859]\n",
      " [(0, 1, 3, 6, 8, 9) 4.745586981786349 0.02937353110744215]\n",
      " [(0, 1, 3, 6, 8, 10) 6.130426523829219 0.013287481164137131]\n",
      " [(0, 1, 4, 5, 6, 7) 11.710180001443998 0.000621591067153163]\n",
      " [(0, 1, 4, 5, 6, 8) 11.606103853822024 0.0006573571797873298]\n",
      " [(0, 1, 4, 5, 6, 9) 4.334990121628703 0.037336632575750234]\n",
      " [(0, 1, 4, 5, 6, 10) 9.894268828618436 0.001657943131832849]\n",
      " [(0, 1, 4, 5, 7, 8) 5.226753791863733 0.022242006363723928]\n",
      " [(0, 1, 4, 6, 7, 8) 18.14480173101883 2.0472751771620222e-05]\n",
      " [(0, 1, 4, 6, 7, 9) 4.71210348229841 0.029950982668205364]\n",
      " [(0, 1, 4, 6, 7, 10) 12.943254402622856 0.00032107634991656545]\n",
      " [(0, 1, 4, 6, 8, 9) 5.399836993208751 0.020138632359627335]\n",
      " [(0, 1, 4, 6, 8, 10) 14.768163450343799 0.00012157076976705628]\n",
      " [(0, 1, 5, 6, 7, 8) 11.36356661918801 0.0007489885722807534]\n",
      " [(0, 1, 5, 6, 7, 9) 8.19978871545581 0.004189526826640785]\n",
      " [(0, 1, 5, 6, 7, 10) 8.62548548922701 0.003314920800293577]\n",
      " [(0, 1, 5, 6, 8, 9) 8.24764560410706 0.004080489224544223]\n",
      " [(0, 1, 5, 6, 8, 10) 8.618609187833183 0.0033274581647113046]\n",
      " [(0, 1, 5, 6, 9, 10) 6.14568788137504 0.013173291429996225]\n",
      " [(0, 1, 6, 7, 8, 9) 9.921345687411073 0.0016337289026367254]\n",
      " [(0, 1, 6, 7, 8, 10) 9.771446933254404 0.0017724290996496315]\n",
      " [(0, 1, 6, 7, 9, 10) 5.874484721541574 0.015361868455362802]\n",
      " [(0, 1, 6, 8, 9, 10) 6.025162662752576 0.014103331914227545]\n",
      " [(0, 2, 3, 8, 9, 10) 4.124695896378522 0.04226161566416426]\n",
      " [(0, 2, 7, 8, 9, 10) 9.95658867141612 0.0016027475960293405]\n",
      " [(0, 4, 5, 6, 7, 8) 7.919576427897449 0.004890272015496546]\n",
      " [(0, 4, 5, 6, 7, 10) 6.144513192355053 0.013182044831730457]\n",
      " [(0, 4, 5, 6, 8, 10) 5.993220844332596 0.014360957383992241]\n",
      " [(0, 4, 6, 7, 8, 10) 6.683826848912254 0.009729153036897708]\n",
      " [(1, 2, 4, 5, 6, 7) 4.603589028719264 0.03190509769778402]\n",
      " [(1, 2, 4, 5, 6, 8) 5.291461191839581 0.021430231667928706]\n",
      " [(1, 2, 4, 6, 7, 8) 5.401434004017408 0.020120213514040634]\n",
      " [(1, 2, 4, 6, 8, 10) 4.2806123574455714 0.03854945768770353]\n",
      " [(1, 2, 5, 6, 7, 8) 7.816471848783589 0.00517721663760601]\n",
      " [(1, 2, 5, 6, 7, 9) 4.366618739530893 0.036649646958851465]\n",
      " [(1, 2, 5, 6, 7, 10) 4.538153291056466 0.03314733379493571]\n",
      " [(1, 2, 5, 6, 8, 9) 5.067774946563905 0.024374651595212054]\n",
      " [(1, 2, 5, 6, 8, 10) 4.843938570463607 0.027743461697472035]\n",
      " [(1, 2, 6, 7, 8, 9) 4.105225807246999 0.042750882307862954]\n",
      " [(1, 2, 6, 7, 8, 10) 4.263267676473363 0.03894493906917982]\n",
      " [(1, 3, 4, 6, 7, 8) 6.1974257102486705 0.012793625735019569]\n",
      " [(1, 3, 4, 6, 7, 9) 4.336088521123471 0.03731255024965904]\n",
      " [(1, 3, 4, 6, 7, 10) 5.3436959880351385 0.020797333573985617]\n",
      " [(1, 3, 4, 6, 8, 9) 4.9452947214324325 0.026161778077944148]\n",
      " [(1, 3, 4, 6, 8, 10) 5.446508740918615 0.01960747295667567]\n",
      " [(1, 3, 5, 6, 7, 8) 4.466063833724762 0.0345745534616802]\n",
      " [(1, 3, 6, 7, 8, 9) 5.428473012350951 0.019810994555847807]\n",
      " [(1, 3, 6, 7, 8, 10) 6.826630412581187 0.008980853537148907]\n",
      " [(1, 3, 6, 7, 9, 10) 4.0191993962467185 0.04498506321561533]\n",
      " [(1, 4, 5, 6, 7, 8) 9.90913650384407 0.001644602634852254]\n",
      " [(1, 4, 5, 6, 7, 9) 4.929603101008906 0.02640038049909858]\n",
      " [(1, 4, 5, 6, 7, 10) 8.404927203583327 0.0037420537988997514]\n",
      " [(1, 4, 5, 6, 8, 9) 5.63279301214868 0.01762752328584453]\n",
      " [(1, 4, 5, 6, 8, 10) 8.269807673225023 0.004030977622437445]\n",
      " [(1, 4, 5, 6, 9, 10) 4.0971258651536075 0.04295617772849135]\n",
      " [(1, 4, 6, 7, 8, 9) 6.284866590759511 0.012177324438071393]\n",
      " [(1, 4, 6, 7, 8, 10) 16.907229469949158 3.925180277044899e-05]\n",
      " [(1, 4, 6, 7, 9, 10) 4.448171589307078 0.03493863557157089]\n",
      " [(1, 4, 6, 8, 9, 10) 5.026194428960681 0.024966699714165874]\n",
      " [(1, 5, 6, 7, 8, 9) 8.86590206655366 0.0029054663821948845]\n",
      " [(1, 5, 6, 7, 8, 10) 8.513196237743944 0.0035258023094153833]\n",
      " [(1, 5, 6, 7, 9, 10) 6.50502139313719 0.010757026982100855]\n",
      " [(1, 5, 6, 8, 9, 10) 6.4141962842540385 0.011321155690253194]\n",
      " [(1, 6, 7, 8, 9, 10) 6.779028874295609 0.009223505884402681]\n",
      " [(2, 3, 7, 8, 9, 10) 5.807249906505593 0.015960234084271364]\n",
      " [(4, 5, 6, 7, 8, 10) 5.130657361019558 0.023506823286095192]]\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "# 1599 red win\n",
    "wine_path = \"./Data/winequality-red.csv\"\n",
    "data = np.genfromtxt(wine_path, delimiter=\";\", skip_header=1)\n",
    "# with open(wine_path) as f:\n",
    "#     f.readline()\n",
    "#     data = np.loadtxt(f, delimiter=\";\")\n",
    "# f = open(wine_path)\n",
    "# f.readline()  # skip the header\n",
    "# data = np.loadtxt(f, delimiter=\";\")\n",
    "# f.close()\n",
    "\n",
    "X_wine = data[:, 0:11]  # select columns 1 through 11\n",
    "y_wine = data[:, 11]\n",
    "\n",
    "# convert y values to 0, 1\n",
    "y_wine = np.where(y_wine < 5.5, 0, 1)\n",
    "# print(y_wine[:100])\n",
    "# wine = list(zip(X_wine, y_wine))\n",
    "# print(\"Total number of wine\", len(wine))\n",
    "# print(X_wine[0])\n",
    "# print(y_wine[0])\n",
    "# print(wine[0])\n",
    "\n",
    "# standardize\n",
    "#X_wine = standardize(X_wine)\n",
    "\n",
    "# adding interaction terms\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "comblist = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "comb = combinations(comblist, 2)\n",
    "\n",
    "i = 11\n",
    "for (a, b) in comb:\n",
    "    # print((a, b), i)\n",
    "    list.append((a, b))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "comb3 = combinations(comblist, 3)\n",
    "for (a, b, c) in comb3:\n",
    "    #print((a, b, c), i)\n",
    "    list.append((a, b, c))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "comb4 = combinations(comblist, 4)\n",
    "for (a, b, c, d) in comb4:\n",
    "    #print((a, b, c, d), i)\n",
    "    list.append((a, b, c, d))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "comb5 = combinations(comblist, 5)\n",
    "for (a, b, c, d, e) in comb5:\n",
    "    #print((a, b, c, d, e), i)\n",
    "    list.append((a, b, c, d, e))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]  * X_wine[:, e]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "\n",
    "comb6 = combinations(comblist, 6)\n",
    "for (a, b, c, d, e, f) in comb6:\n",
    "    #print((a, b, c, d, e, f), i)\n",
    "    list.append((a, b, c, d, e, f))\n",
    "    i += 1\n",
    "    inter = X_wine[:, a] * X_wine[:, b] * X_wine[:, c] * X_wine[:, d]  * X_wine[:, e] * X_wine[:, f]\n",
    "    X_wine = np.column_stack((X_wine, inter))\n",
    "    \n",
    "\n",
    "# normalize\n",
    "X_wine = normalize(X_wine)\n",
    "\n",
    "# chi squared table todo\n",
    "chi, pval = chi2(X_wine, y_wine)\n",
    "list = np.asarray(list)\n",
    "chilist = np.column_stack((list, chi, pval))\n",
    "\n",
    "# threshold of chisquare interaction\n",
    "index_of_chi = np.where(chi < 4)\n",
    "#keep no interaction terms\n",
    "index_of_chi = np.delete(index_of_chi, np.where(index_of_chi[0] < 11), axis=1)\n",
    "# remove comment to delete insignificant interaction rows\n",
    "X_wine = np.delete(X_wine, index_of_chi , axis=1)\n",
    "chilist = np.delete(chilist, index_of_chi, axis=0)\n",
    "print(chilist)\n",
    "print(len(chilist))\n",
    "\n",
    "# variance threshold on features\n",
    "# print(X_wine.var(axis=0))\n",
    "# for lda removing features decreased accuracy\n",
    "# index_of_var = np.where(X_wine.var(axis=0) < 0.00000001)\n",
    "# X_wine = np.delete(X_wine, index_of_var[0] , axis=1)\n",
    "\n",
    "\n",
    "# recombininng and shuffling\n",
    "wine = np.column_stack((X_wine, y_wine))\n",
    "#np.random.shuffle(wine)\n",
    "\n",
    "# split into train test set\n",
    "n = X_wine.shape[0]\n",
    "f_size = X_wine.shape[1]\n",
    "X_wine = wine[(n // 10) :, 0:f_size]\n",
    "X_wine_test = wine[: (n // 10), 0:f_size]\n",
    "y_wine = wine[(n // 10) :, f_size].reshape(-1,1)\n",
    "y_wine_test = wine[: (n // 10), f_size].reshape(-1,1)\n",
    "\n",
    "# standardize\n",
    "\n",
    "#X_wine = normalize(X_wine)\n",
    "#X_wine_test= normalize(X_wine_test)\n",
    "\n",
    "\n",
    "# qualities = [float(item[-1]) for item in wines[1:]]\n",
    "# sum(qualities) / len(qualities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  1483\n",
      "0.6978472222222223\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  819\n",
      "0.7415277777777779\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  559\n",
      "0.751736111111111\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  427\n",
      "0.7516666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  330\n",
      "0.7491666666666668\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features: \",len(chilist))\n",
    "# kfold test\n",
    "score = []\n",
    "scorec = []\n",
    "for i in range(10):\n",
    "    for train_index, test_index in kfold_index(5, X_wine):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_wine[train_index], X_wine[test_index]\n",
    "        y_train, y_test = y_wine[train_index], y_wine[test_index]\n",
    "        # project lda\n",
    "        ld = LogReg(learning_rate=0.001)\n",
    "        #ld = LogisticRegression(penalty = 'none', solver = 'sag', random_state=0, C = 1000)\n",
    "        \n",
    "        ld.fit(X_train, y_train)\n",
    "\n",
    "        # scikit learn lda\n",
    "        #clf = LinearDiscriminantAnalysis()\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_predT = clf.predict(X_test)\n",
    "        #scorec.append(evaluate_acc(y_test, y_predT))\n",
    "\n",
    "        y_pred = ld.predict(X_test)\n",
    "        score.append(evaluate_acc(y_test, y_pred))\n",
    "\n",
    "print(np.mean(score))\n",
    "#print(np.mean(scorec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
